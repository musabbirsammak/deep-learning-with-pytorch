{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "A tensor is essentially a matrix that can have multiple discrete dimensions. For example, a 2D tensor can represent a grayscale or black and white image. Similarly, a 3D tensor can represent a multi-channel color image. Lastly, a 4D tensor can represent a sequence of images.\n",
    "\n",
    "Among many of the features that PyTorch tensors provide, some important ones are as follows:\n",
    "- Efficient computation on both CPU and GPU (*NumPy doesn't support GPU*)\n",
    "- Automatic differentiation (*NumPy doesn't have this capability*)\n",
    "- Efficient data Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# fastai is a higher level library for PyTorch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check PyTorch Version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can run both in CPU and GPU. `torch.cuda.is_available()` is a convenient function to check if your environment supports GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Tensor` can be created from a Python list, with a designated data type, and on a specified device. The `requires_grad` tells whether to compute gradients for any operation with this variable. We will explore this autograd functionality later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.], device='cuda:0', dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64, device=device, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instances of a `Tensor` object has the following usefult properties.\n",
    "- `ndim` returns the number of *axes* or *dimensions* of the tensor. It is also called the *rank* of a tensor. For example, the above tensor has rank of 1 since it has only 1 axis. In contrast, a $5\\times10$ tensor will have a rank of 2 since it has 2 axes (rows and columns).\n",
    "- `shape` returns the length of each axis or rank. For example, a $15\\times10\\times5$ tensor has 3 dimensions or axes, and the shape of each axes are 15, 10, and 5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of elements:  5\n",
      "Data type of each element:  torch.float64\n",
      "Rank/Dimension:  1\n",
      "Length of each dimension:  torch.Size([5])\n",
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Total number of elements: ', x.numel())\n",
    "print('Data type of each element: ', x.dtype)\n",
    "print('Rank/Dimension: ', x.ndim)\n",
    "print('Length of each dimension: ', x.shape)\n",
    "print(x.requires_grad)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tensor()` Vs. `as_tensor()` Vs. `from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tensor()` always copies data and discards and previous autograd history.\n",
    "- If you do not want to copy data and share autograd history, you may use `torch.as_tensor()`.\n",
    "- `torch.from_numpy()` creates a tensor that shares storage with a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(5, dtype=np.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.from_numpy(y)\n",
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification to `z` will be reflected on `y` and vice-versa since they share memory storage now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[100.75   1.     2.     3.     4.  ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 100.75\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if you use the `tensor` function, the values of y is copied to z and they are not using the same memory address. So, any change in z will not be reflected in y and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([500.7500,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[100.75   1.     2.     3.     4.  ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 500.75\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if your use the `as_tensor` function, the values of y is not copied to z and they are using the ame memory address. So, any change in z will be reflected in y and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.as_tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident below, both `y` and `z` have different memory addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([125.9000,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[125.9   1.    2.    3.    4. ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 125.90\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert a `tensor` back to a `ndarray` using the `numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125.9,   1. ,   2. ,   3. ,   4. ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a `tensor` from a `Pandas` `Series` data structure. We can simply convert a `Series` into a `ndarray` using the `values` property. Then we can simply use the `as_tensor` or `from_numpy` function to create a new `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "January     1\n",
       "February    2\n",
       "March       3\n",
       "April       4\n",
       "May         5\n",
       "June        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series([1, 2, 3, 4, 5, 6],\n",
    "                    index=['January', 'February', 'March', 'April', 'May', 'June'])\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(series.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorthands for Tensor Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several useful shorthands for quickly creating tensors of arbitrary shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty((3, 3))\n",
    "y = torch.zeros((2, 2))\n",
    "z = torch.ones((3, 3))\n",
    "p = torch.rand((2, 2))\n",
    "q = torch.eye(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful function for plotting mathematical functions is <code>torch.linspace()</code> and `torch.arange()`. <code>torch.linspace()</code> returns evenly spaced numbers over a specified interval. You specify the starting point of the sequence and the ending point of the sequence. The parameter <code>steps</code> indicates the number of samples to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(start=10, end=20, step=5)\n",
    "y = torch.linspace(start=10, end=20, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 15])\n",
      "tensor([10.0000, 12.5000, 15.0000, 17.5000, 20.0000])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `tolist` function to convert a `tensor` directly to a `Python` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 12.5, 15.0, 17.5, 20.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting\n",
    "Sometimes you might need to change the data type of a tensor that you have already created. There are two different ways to that in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.bool())\n",
    "# convert to int16\n",
    "print(x.short())\n",
    "# convert to int64\n",
    "print(x.long())\n",
    "# convert to float16\n",
    "print(x.half())\n",
    "# convert to float32\n",
    "print(x.float())\n",
    "# convert to float64\n",
    "print(x.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to typecast using the `to` function, and giving the intended `torch` type as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.to(torch.bool))\n",
    "# convert to int16\n",
    "print(x.to(torch.int16))\n",
    "# convert to int64\n",
    "print(x.to(torch.int64))\n",
    "# convert to float16\n",
    "print(x.to(torch.float16))\n",
    "# convert to float32\n",
    "print(x.to(torch.float32))\n",
    "# convert to float64\n",
    "print(x.to(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([-3, -3, -3])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# addition\n",
    "z = x + y\n",
    "print(z)\n",
    "# subtraction\n",
    "z = x - y\n",
    "print(z)\n",
    "# element-wise division\n",
    "z = x / y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# inplace operations, more efficient\n",
    "x.add_(y)\n",
    "print(x)\n",
    "x.subtract_(y)\n",
    "print(x)\n",
    "x.pow_(2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3561, 0.4200, 0.0603, 0.1983, 0.4356],\n",
       "        [0.7158, 0.2646, 0.1930, 0.1444, 0.4683],\n",
       "        [0.8573, 0.3876, 0.2224, 0.2040, 0.6106],\n",
       "        [0.0629, 0.0137, 0.0181, 0.0085, 0.0344]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "x = torch.rand((4, 2))\n",
    "y = torch.rand((2, 5))\n",
    "x.mm(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 30])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch matrix multiplication\n",
    "batch = 64\n",
    "x = torch.rand((batch, 10, 20))\n",
    "y = torch.rand((batch, 20, 30))\n",
    "x.bmm(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 14, 24, 36])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "x = torch.arange(5)\n",
    "y = torch.arange(5, 10)\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Operataions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7118, 0.1709, 0.5354, 0.2788, 0.6079])\n",
      "tensor([0.3637, 0.1844, 0.2946, 0.8069, 0.5260])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5))\n",
    "y = torch.rand((5))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False,  True, False])\n",
      "tensor([ True, False,  True, False,  True])\n",
      "tensor([False, False, False, False, False])\n",
      "tensor([True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(x < y)\n",
    "print(x > y)\n",
    "print(x == y)\n",
    "print(x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((x != y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "PyTorch, when it tries to perform a simple operation between two tensors of different ranks, will use broadcasting: it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. For example, you can not add a $[3\\times 3]$ matrix with a $[3\\times 4]$ matrix beacuse the later has an extra column and there is nothing left to add with it after you have added the first $3$ with each other. However, if you add a $[3\\times 3]$ matrix with a $[1\\times 3]$ vector, you can add the vector with each row of the matrix. Here, the $[1\\times 3]$ vector, which has smaller rank, is expanded to $[3\\times 3]$ matrix by repeating the $[1\\times 3]$ vector $3$ times. Consider the following image.\n",
    "\n",
    "![Broadcasting](./images/broadcasting.png \"Broadcasting\")\n",
    "\n",
    "<small>This image was taken from **Python Data Science Handbook: Essential Tools for Working with Data 1st Edition by Jake VanderPlas**.</small>\n",
    "\n",
    "### Boradcasting with a Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [3, 4, 5],\n",
    "    [5, 6, 7]\n",
    "])\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the shape of $t$ is $[3\\times 3]$. Now, if we substract $5$, which is a scalar, from $t$, PyTorch will consider a matrix of $5$ of the same shape $[3\\times 3]$ as $t$ like the following\n",
    "$$\n",
    "[\n",
    "    [5, 5, 5],\\\\\n",
    "    [5, 5, 5],\\\\\n",
    "    [5, 5, 5]\n",
    "]\n",
    "$$\n",
    "and substract it from $t$ using the normal elementwise substraction. **However, PyTorch will never actually create this matrix so that it can save memory.** As you can see below, $5$ is substracted from each element of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4, -3, -2],\n",
       "        [-2, -1,  0],\n",
       "        [ 0,  1,  2]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t - 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting with a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [5, 6, 7]])\n",
      "Number of dimensions:  2\n",
      "Length of each dimension:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print('Tensor: ', t)\n",
    "print('Number of dimensions: ', t.ndim)\n",
    "print('Length of each dimension: ', t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:  tensor([[1, 2, 3]])\n",
      "Number of dimensions:  2\n",
      "Length of each dimension:  torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1, 2, 3]])\n",
    "print('Vector: ', v)\n",
    "print('Number of dimensions: ', v.ndim)\n",
    "print('Length of each dimension: ', v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add the vector $v$ to the matrix $t$, the vector will be expanded to match the dimension of the matrix $t$. The vector will act like a matrix of the same shape of $t$ like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.expand_as(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add it with matrix $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 4,  6,  8],\n",
       "        [ 6,  8, 10]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + v.expand_as(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boradcasting does this expansion automatically whenever you are performing any mathematical operation between tensors that have different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 4,  6,  8],\n",
       "        [ 6,  8, 10]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Rule\n",
    "When operating on two tensors, PyTorch compares their shapes elementwise. It starts with the trailing dimensions and works its way backward, adding $1$ when it meets empty dimensions. Two dimensions are compatible when one of the following is true:\n",
    "- They are equal.\n",
    "- One of them is $1$, in which case that dimension is broadcast to make it the same as the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x:  4\n",
      "Length of each dimension of x:  torch.Size([64, 3, 256, 256])\n",
      "\n",
      "Dimension of y:  2\n",
      "Length of each dimension of y:  torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((64, 3, 256, 256))\n",
    "y = torch.rand((256, 256))\n",
    "\n",
    "print('Dimension of x: ', x.ndim)\n",
    "print('Length of each dimension of x: ', x.shape)\n",
    "print()\n",
    "print('Dimension of y: ', y.ndim)\n",
    "print('Length of each dimension of y: ', y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start comparing the lengths of each dimension from the end. In the last dimension, both $x$ and $y$ have $256$. The same is true for the $2^{nd}$ from the last dimension. After that, $x$ has extra $2$ dimensions which $y$ does not have, but we can simply add two extra dimensions in $y$ using the `unsqueeze` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "y = y.unsqueeze(0)\n",
    "print(y.shape)\n",
    "y = y.unsqueeze(0)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the lengths of each dimension in both $x$ and $y$ are either same or one of them is $1$. So, our rule holds. We can perform any mathematical operation between them as we like. **However, PyTorch does these `unsqueeze` operations automatically.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions\n",
    "**It is often better to avoid loops and use these functions to speed up performance.\n",
    "Raw Python loops are too slow. To levelrage the underlying `C` codes of `PyTorch`,\n",
    "avoid them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  9, 11, 13, 15])\n",
      "tensor([15, 40])\n"
     ]
    }
   ],
   "source": [
    "# row-wise sum\n",
    "print(x.sum(dim=0))\n",
    "# column-wise sum\n",
    "print(x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([1, 2, 3, 4, 5]),\n",
      "indices=tensor([0, 0, 0, 0, 0]))\n",
      "\n",
      "torch.return_types.min(\n",
      "values=tensor([1, 6]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# returns the minimum element of each column\n",
    "print(x.min(dim=0))\n",
    "print()\n",
    "# returns the minimum element of each row\n",
    "print(x.min(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([ 5, 10]),\n",
      "indices=tensor([4, 4]))\n",
      "\n",
      "torch.return_types.max(\n",
      "values=tensor([ 6,  7,  8,  9, 10]),\n",
      "indices=tensor([1, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# returns the maximum element of each row\n",
    "print(x.max(dim=1))\n",
    "print()\n",
    "# returns the maximum element of each column\n",
    "print(x.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1])\n",
      "tensor([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the maximum element row-wise\n",
    "print(x.argmax(dim=0))\n",
    "# returns the position of the maximum element column-wise\n",
    "print(x.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the minimum element row-wise\n",
    "print(x.argmin(dim=0))\n",
    "# returns the position of the minimum element column-wise\n",
    "print(x.argmin(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 4.5000, 5.5000, 6.5000, 7.5000])\n"
     ]
    }
   ],
   "source": [
    "# returns means of each column\n",
    "print(torch.mean(x.float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8941, 0.7084, 0.3669, 0.9765, 0.6829, 0.5882, 0.6227, 0.7009, 0.1127,\n",
      "        0.3972])\n",
      "tensor([0.1127, 0.3669, 0.3972, 0.5882, 0.6227, 0.6829, 0.7009, 0.7084, 0.8941,\n",
      "        0.9765])\n",
      "tensor([8, 2, 9, 5, 6, 4, 7, 1, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "# sorts tensor\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x, indices = x.sort(descending=False)\n",
    "print(x)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  4,  5,  6,  7, 10, 11])\n",
      "tensor([ 2,  2,  4,  5,  6,  7, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# clamp tensor between values\n",
    "x = torch.tensor([0, 1, 4, 5, 6, 7, 10, 11])\n",
    "print(x)\n",
    "# any element less than 2 is set to 2, and any element more than 10 is set to 10\n",
    "x = x.clamp(min=2, max=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 0, 1, 1, 1], dtype=torch.bool)\n",
    "# checks if any of the values is true\n",
    "print(x.any())\n",
    "# checks if all the values are true\n",
    "print(x.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "The contents of a tensor can be accessed and modified using Pythonâ€™s indexing and slicing notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "feature_size = (3, 256, 256)\n",
    "\n",
    "img = torch.rand((batch_size, *feature_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# selecting first image\n",
    "print(img[0].shape)\n",
    "# selecting first color channel of first image\n",
    "print(img[0, 0].shape)\n",
    "# selecting first row of first color channel of first image\n",
    "print(img[0, 0, 0].shape)\n",
    "# slecting first column of first color channel of first image\n",
    "print(img[0, 0, :, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number on the left side of the colon represents the index of the first value. The number on the right side of the colon is always 1 larger than the index of the last value. For example, <code>tensor_sample\\[1:4]</code> means you get values from the index 1 to index 3 <i>(4-1)</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# selecting 3rd column of 2nd color channel of all images\n",
    "print(img[:, 1, :, 2].shape)\n",
    "# selecting 3rd column of 2nd color channel of first 10 images\n",
    "print(img[:10, 1, :, 2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage fancy indexing and use boolean conditions to select specific values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True, False, False])\n",
      "tensor([1, 2, 3])\n",
      "tensor([ True, False, False, False,  True])\n",
      "tensor([1, 5])\n",
      "tensor([ True, False, False, False, False])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(x < 4)\n",
    "print(x[x < 4])\n",
    "print((x < 2) | (x > 4))\n",
    "print(x[(x < 2) | (x > 4)])\n",
    "print((x < 3) & (x < 2))\n",
    "print(x[(x < 3) & (x < 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `where` function returns elements as it is if condition is met, otherwise changes value according to given formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1, 102, 103, 104, 105])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.where(x < 2, x + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique` function returns the unique elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.Tensor.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a single item\n",
    "x[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can select a specific value or a range of values from a `tensor`, we can also assign new values to those positions. For example, below we have selected the first 2 items of `x` and assigned new values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 101,   3,   4,   5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2] = torch.tensor([100, 101])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping & Resizing\n",
    "For some operations, the input tensors need to have a certain number of dimensions (also called rank) and a certain number of elements (shape). So, we might have to change the shape of a tensor, add a new dimension, or make a dimension smaller that isn't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `view` function can be used to reshape a vector. For example, before `x` was a 1 dimensional vector. Later we reshaped it into a 2 dimensional matrix with 3 rows and 4 columns. The number of elements in a tensor must remain constant after applying view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.view(3, 4)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x_reshaped.shape)\n",
    "print(x_reshaped.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, `view` requires contiguous memory, in contrast to `reshape`. Therefore, `reshape` is safe to use in expanse of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a tensor with dynamic size, you can use `-1` to represent any size. But you can set only one dimension as `-1`. It means that `PyTorch` will calculate the suitable number for that dimension by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate two tensors both row-wise and column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates two tensors\n",
    "x = torch.rand((3, 3))\n",
    "y = torch.rand((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5010, 0.8989, 0.1619],\n",
      "        [0.1088, 0.6318, 0.7611],\n",
      "        [0.6168, 0.1788, 0.2481],\n",
      "        [0.6397, 0.5900, 0.2518],\n",
      "        [0.4200, 0.1559, 0.5479],\n",
      "        [0.8271, 0.1436, 0.7733]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y column-wise\n",
    "print(torch.cat((x, y), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5010, 0.8989, 0.1619, 0.6397, 0.5900, 0.2518],\n",
      "        [0.1088, 0.6318, 0.7611, 0.4200, 0.1559, 0.5479],\n",
      "        [0.6168, 0.1788, 0.2481, 0.8271, 0.1436, 0.7733]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y row-wise\n",
    "print(torch.cat((x, y), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide a `tensor` into multiple chunks using the `chunk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5010, 0.8989, 0.1619]])\n",
      "tensor([[0.1088, 0.6318, 0.7611]])\n",
      "tensor([[0.6168, 0.1788, 0.2481]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.chunk(x, 3)\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might need splits of different sizes. We can use the `split` function instead and define the size of each splits manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5010, 0.8989, 0.1619],\n",
      "        [0.1088, 0.6318, 0.7611]])\n",
      "tensor([[0.6168, 0.1788, 0.2481]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.split(x, split_size_or_sections=[2, 1])\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5010, 0.8989, 0.1619, 0.1088, 0.6318, 0.7611, 0.6168, 0.1788, 0.2481])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrolling all elements\n",
    "x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "torch.Size([64, 196608])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "# converting 2d color images into a vector of pixels\n",
    "print(img.reshape(64, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "# swapping dimensions, changing colour channel dimension to the last\n",
    "print(img.permute(0, 2, 3, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `squeeze` function removes the given dimension, whereas the `unsqueeze` function adds an extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 1])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# add and remove a single dimension to the existing one\n",
    "x = torch.arange(10)\n",
    "print(x)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(2)\n",
    "print(x.shape)\n",
    "x = x.squeeze(0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classification\n",
    "In this short exercise, we will be building a naive **baseline** classifier to classify digits $3$ and $7$. We will utilize the `fast.ai` repository's MNIST SAMPLE dataset that only contains digits $3$ and $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/musab/.fastai/data/mnist_sample')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads MNIST dataset from Fast.ai repository\n",
    "# This trimmed dataset contains only 3s and 7s\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast.ai API returns the path to the downloaded dataset. The path is not just a string, rather it is a python Path object Learn more from [**documentation**](https://docs.python.org/3/library/pathlib.html). Or watch this [**tutorial**](https://www.youtube.com/watch?v=YwhOUyTxXVE) for a quick introduction. The new path object comes with many unix commands baked into itself for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/labels.csv'), Path('C:/Users/musab/.fastai/data/mnist_sample/train'), Path('C:/Users/musab/.fastai/data/mnist_sample/valid')]\n"
     ]
    }
   ],
   "source": [
    "print(path.ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `/` operator is one of the most convenient shorthands for creating new paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/train/3'), Path('C:/Users/musab/.fastai/data/mnist_sample/train/7')]\n"
     ]
    }
   ],
   "source": [
    "print((path / 'train').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/train/3/10.png'), Path('C:/Users/musab/.fastai/data/mnist_sample/train/3/10000.png'), Path('C:/Users/musab/.fastai/data/mnist_sample/train/3/10011.png')]\n",
      "\n",
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/train/7/10002.png'), Path('C:/Users/musab/.fastai/data/mnist_sample/train/7/1001.png'), Path('C:/Users/musab/.fastai/data/mnist_sample/train/7/10014.png')]\n"
     ]
    }
   ],
   "source": [
    "threes = (path/'train/3').ls().sorted()\n",
    "sevens = (path/'train/7').ls().sorted()\n",
    "\n",
    "print(threes[:3])\n",
    "print()\n",
    "print(sevens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131\n",
      "6265\n"
     ]
    }
   ],
   "source": [
    "print(len(threes))\n",
    "print(len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "\n",
    "len(three_tensors), len(seven_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images((three_tensors[15], seven_tensors[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes = torch.stack(three_tensors).float() / 255\n",
    "\n",
    "print(stacked_threes.shape)\n",
    "print(stacked_sevens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to find the mean representation of $3$ and $7$. Then for any new digit,\n",
    "we will compute the distance from the digit to both $3$ and $7$. We will predict the\n",
    "digit with the smallest distance between the two. Next, we will calculate the average\n",
    "representation of the digits $3$ and $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create some functions to calculate distance between two images, and\n",
    "also to predict digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance(img1: tensor, img2: tensor) -> tensor:\n",
    "    \"\"\"Function to calculate average l1 distance or the mean absolute error between\n",
    "    two images.\"\"\"\n",
    "    return (img1 - img2).abs().mean((-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img: tensor) -> int:\n",
    "    \"\"\"Returns 3 if the distance between given image and average image of the\n",
    "    digit 3 is smaller than that of digit 7. Returns 7 otherwise.\"\"\"\n",
    "    mask = l1_distance(img, mean3) < l1_distance(img, mean7)\n",
    "    mask = torch.where(mask, torch.tensor(3), torch.tensor(7))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "a_7 = stacked_sevens[12]\n",
    "\n",
    "show_images((a_3, a_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(predict(a_3))\n",
    "print(predict(a_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model worked well on the training data. Let's load the validation data and \n",
    "measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float() / 255\n",
    "\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]) \n",
    "valid_7_tens = valid_7_tens.float() / 255\n",
    "\n",
    "valid_3_tens.shape, valid_7_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will combine the $3$ and $7$ tensors to a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 28, 28])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_x = torch.cat([valid_3_tens, valid_7_tens])\n",
    "valid_data_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a tensor to store the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_y = torch.tensor([3] * valid_3_tens.shape[0] + [7] * valid_7_tens.shape[0])\n",
    "valid_data_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will predict and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(valid_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.14229583740234"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = torch.sum(predictions == valid_data_y) / len(predictions) * 100\n",
    "acc.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
