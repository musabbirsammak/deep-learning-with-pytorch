{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "A tensor is essentially a matrix that can have multiple discrete dimensions. For example, a 2D tensor can represent a grayscale or black and white image. Similarly, a 3D tensor can represent a multi-channel color image. Lastly, a 4D tensor can represent a sequence of images.\n",
    "\n",
    "Among many of the features that PyTorch tensors provide, some important ones are as follows:\n",
    "- Efficient computation on both CPU and GPU (*NumPy doesn't support GPU*)\n",
    "- Automatic differentiation (*NumPy doesn't have this capability*)\n",
    "- Efficient data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# fastai is a higher level library for PyTorch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check PyTorch Version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can run both in CPU and GPU. `torch.cuda.is_available()` is a convenient function to check if your environment supports GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# setting device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Tensor` can be created from a Python list, with a designated data type, and on a specified device. The `requires_grad` tells whether to compute gradients for any operation with this variable. We will explore this autograd functionality later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64, device=device, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instances of a `Tensor` object has the following usefult properties.\n",
    "- `ndim` returns the number of *axes* or *dimensions* of the tensor. It is also called the *rank* of a tensor. For example, the above tensor has rank of 1 since it has only 1 axis. In contrast, a $5\\times10$ tensor will have a rank of 2 since it has 2 axes (rows and columns).\n",
    "- `shape` returns the length of each axis or rank. For example, a $15\\times10\\times5$ tensor has 3 dimensions or axes, and the shape of each axes are 15, 10, and 5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of elements:  5\n",
      "Data type of each element:  torch.float64\n",
      "Rank/Dimension:  1\n",
      "Length of each dimension:  torch.Size([5])\n",
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print('Total number of elements: ', x.numel())\n",
    "print('Data type of each element: ', x.dtype)\n",
    "print('Rank/Dimension: ', x.ndim)\n",
    "print('Length of each dimension: ', x.shape)\n",
    "print(x.requires_grad)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tensor()` Vs. `as_tensor()` Vs. `from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tensor()` always copies data and discards and previous autograd history.\n",
    "- If you do not want to copy data and share autograd history, you may use `torch.as_tensor()`.\n",
    "- `torch.from_numpy()` creates a tensor that shares storage with a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(5, dtype=np.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.from_numpy(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification to `z` will be reflected on `y` since they share memory storage now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[100.75   1.     2.     3.     4.  ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 100.75\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of y is copied to z. They are using the same memory address. So, any change in z will be reflected in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([500.7500,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[100.75   1.     2.     3.     4.  ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 500.75\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of y is not copied to z. They are not using the ame memory address. So, any change in z will not be reflected in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.as_tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident below, both `y` and `z` have different memory addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([125.9000,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[125.9   1.    2.    3.    4. ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 125.90\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert a `tensor` back to a `ndarray` using the `numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125.9,   1. ,   2. ,   3. ,   4. ], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a `tensor` from a `Pandas` `Series` data structure. We can simply convert a `Series` into a `ndarray` using the `values` property. Then we can simply use the `as_tensor` or `from_numpy` function to create a new `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "January     1\n",
       "February    2\n",
       "March       3\n",
       "April       4\n",
       "May         5\n",
       "June        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series([1, 2, 3, 4, 5, 6],\n",
    "                    index=['January', 'February', 'March', 'April', 'May', 'June'])\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(series.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorthands for Tensor Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several useful shorthands for quickly creating tensors of arbitrary shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty((3, 3))\n",
    "y = torch.zeros((2, 2))\n",
    "z = torch.ones((3, 3))\n",
    "p = torch.rand((2, 2))\n",
    "q = torch.eye(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2612e-44, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.7551e-40, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further methods for creating `tensor`s. A useful function for plotting mathematical functions is <code>torch.linspace()</code>. <code>torch.linspace()</code> returns evenly spaced numbers over a specified interval. You specify the starting point of the sequence and the ending point of the sequence. The parameter <code>steps</code> indicates the number of samples to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(start=10, end=20, step=5)\n",
    "y = torch.linspace(start=10, end=20, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 15])\n",
      "tensor([10.0000, 12.5000, 15.0000, 17.5000, 20.0000])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `tolist` function to convert a `tensor` directly to a `Python` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 12.5, 15.0, 17.5, 20.0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting\n",
    "Sometimes you might need to change the data type of a tensor that you have already created. There are two different ways to that in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.bool())\n",
    "# convert to int16\n",
    "print(x.short())\n",
    "# convert to int64\n",
    "print(x.long())\n",
    "# convert to float16\n",
    "print(x.half())\n",
    "# convert to float32\n",
    "print(x.float())\n",
    "# convert to float64\n",
    "print(x.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to typecast using the `to` function, and giving the intended `torch` type as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.to(torch.bool))\n",
    "# convert to int16\n",
    "print(x.to(torch.int16))\n",
    "# convert to int64\n",
    "print(x.to(torch.int64))\n",
    "# convert to float16\n",
    "print(x.to(torch.float16))\n",
    "# convert to float32\n",
    "print(x.to(torch.float32))\n",
    "# convert to float64\n",
    "print(x.to(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([-3, -3, -3])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# addition\n",
    "z = x + y\n",
    "print(z)\n",
    "# subtraction\n",
    "z = x - y\n",
    "print(z)\n",
    "# element-wise division\n",
    "z = x / y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# inplace operations, more efficient\n",
    "x.add_(y)\n",
    "print(x)\n",
    "x.subtract_(y)\n",
    "print(x)\n",
    "x.pow_(2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9703, 0.0894, 0.9128, 0.7919, 1.0077],\n",
       "        [0.4227, 0.0311, 0.4136, 0.3095, 0.4387],\n",
       "        [0.1663, 0.0476, 0.0907, 0.2822, 0.1738],\n",
       "        [0.6060, 0.0606, 0.5604, 0.5162, 0.6295]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "x = torch.rand((4, 2))\n",
    "y = torch.rand((2, 5))\n",
    "x.mm(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 30])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch matrix multiplication\n",
    "batch = 64\n",
    "x = torch.rand((batch, 10, 20))\n",
    "y = torch.rand((batch, 20, 30))\n",
    "x.bmm(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 14, 24, 36])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "x = torch.arange(5)\n",
    "y = torch.arange(5, 10)\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Operataions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5965, 0.7356, 0.3697, 0.8561, 0.5520])\n",
      "tensor([0.5431, 0.0540, 0.6521, 0.3334, 0.4482])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5))\n",
    "y = torch.rand((5))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True, False, False])\n",
      "tensor([ True,  True, False,  True,  True])\n",
      "tensor([False, False, False, False, False])\n",
      "tensor([True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(x < y)\n",
    "print(x > y)\n",
    "print(x == y)\n",
    "print(x != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((x != y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "PyTorch, when it tries to perform a simple operation between two tensors of different ranks, will use broadcasting: it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((5, 5))\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 4., 5., 6.],\n",
       "        [2., 3., 4., 5., 6.],\n",
       "        [2., 3., 4., 5., 6.],\n",
       "        [2., 3., 4., 5., 6.],\n",
       "        [2., 3., 4., 5., 6.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5) vector is broadcasted to each row of (5, 5) matrix.\n",
    "# PyTorch pretends that the (5) vector is actually (5, 5)\n",
    "# and fills the other 4 rows using the first row. But it\n",
    "# doesn't actually allocate them in memory.\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions\n",
    "**It is often better to avoid loops and use these functions to speed up performance.\n",
    "Raw Python loops are too slow. To levelrage the underlying `C` codes of `PyTorch`,\n",
    "avoid them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  9, 11, 13, 15])\n",
      "tensor([15, 40])\n"
     ]
    }
   ],
   "source": [
    "# row-wise sum\n",
    "print(x.sum(dim=0))\n",
    "# column-wise sum\n",
    "print(x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([1, 2, 3, 4, 5]),\n",
      "indices=tensor([0, 0, 0, 0, 0]))\n",
      "torch.return_types.min(\n",
      "values=tensor([1, 6]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# returns the minimum element of each column\n",
    "print(x.min(dim=0))\n",
    "# returns the minimum element of each row\n",
    "print(x.min(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([ 5, 10]),\n",
      "indices=tensor([4, 4]))\n",
      "torch.return_types.max(\n",
      "values=tensor([ 6,  7,  8,  9, 10]),\n",
      "indices=tensor([1, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# returns the maximum element of each row\n",
    "print(x.max(dim=1))\n",
    "# returns the maximum element of each column\n",
    "print(x.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4])\n",
      "tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the maximum element row-wise\n",
    "print(x.argmax(dim=1))\n",
    "# returns the position of the maximum element column-wise\n",
    "print(x.argmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0])\n",
      "tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the minimum element row-wise\n",
    "print(x.argmin(dim=1))\n",
    "# returns the position of the minimum element column-wise\n",
    "print(x.argmin(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 4.5000, 5.5000, 6.5000, 7.5000])\n"
     ]
    }
   ],
   "source": [
    "# returns means of each column\n",
    "print(torch.mean(x.float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6842, 0.8863, 0.8230, 0.9111, 0.4031, 0.2792, 0.8030, 0.4076, 0.8587,\n",
      "        0.7952])\n",
      "tensor([0.2792, 0.4031, 0.4076, 0.6842, 0.7952, 0.8030, 0.8230, 0.8587, 0.8863,\n",
      "        0.9111])\n",
      "tensor([5, 4, 7, 0, 9, 6, 2, 8, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# sorts tensor\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x, indices = x.sort(descending=False)\n",
    "print(x)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  4,  5,  6,  7, 10, 11])\n",
      "tensor([ 2,  2,  4,  5,  6,  7, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# clamp tensor between values\n",
    "x = torch.tensor([0, 1, 4, 5, 6, 7, 10, 11])\n",
    "print(x)\n",
    "# any element less than 2 is set to 2, and any element more than 10 is set to 10\n",
    "x = x.clamp(min=2, max=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 0, 1, 1, 1], dtype=torch.bool)\n",
    "# checks if any of the values is true\n",
    "print(x.any())\n",
    "# checks if all the values are true\n",
    "print(x.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "The contents of a tensor can be accessed and modified using Pythonâ€™s indexing and slicing notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "feature_size = (3, 256, 256)\n",
    "\n",
    "img = torch.rand((batch_size, *feature_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# selecting first image\n",
    "print(img[0].shape)\n",
    "# selecting first color channel of first image\n",
    "print(img[0, 0].shape)\n",
    "# selecting first row of first color channel of first image\n",
    "print(img[0, 0, 0].shape)\n",
    "# slecting first column of first color channel of first image\n",
    "print(img[0, 0, :, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number on the left side of the colon represents the index of the first value. The number on the right side of the colon is always 1 larger than the index of the last value. For example, <code>tensor_sample\\[1:4]</code> means you get values from the index 1 to index 3 <i>(4-1)</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# selecting 3rd column of 2nd color channel of all images\n",
    "print(img[:, 1, :, 2].shape)\n",
    "# selecting 3rd column of 2nd color channel of first 10 images\n",
    "print(img[:10, 1, :, 2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage fancy indexing and use boolean conditions to select specific values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True, False, False])\n",
      "tensor([1, 2, 3])\n",
      "tensor([ True, False, False, False,  True])\n",
      "tensor([1, 5])\n",
      "tensor([ True, False, False, False, False])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(x < 4)\n",
    "print(x[x < 4])\n",
    "print((x < 2) | (x > 4))\n",
    "print(x[(x < 2) | (x > 4)])\n",
    "print((x < 3) & (x < 2))\n",
    "print(x[(x < 3) & (x < 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `where` function returns elements as it is if condition is met, otherwise changes value according to given formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1, 102, 103, 104, 105])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.where(x < 2, x + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique` function returns the unique elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.Tensor.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a single item\n",
    "x[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can select a specific value or a range of values from a `tensor`, we can also assign new values to those positions. For example, below we have selected the first 2 items of `x` and assigned new values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 101,   3,   4,   5])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2] = torch.tensor([100, 101])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping & Resizing\n",
    "For some operations, the input tensors need to have a certain number of dimensions (also called rank) and a certain number of elements (shape). So, we might have to change the shape of a tensor, add a new dimension, or make a dimension smaller that isn't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `view` function can be used to reshape a vector. For example, before `x` was a 1 dimensional vector. Later we reshaped it into a 2 dimensional matrix with 3 rows and 4 columns. The number of elements in a tensor must remain constant after applying view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.view(3, 4)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x_reshaped.shape)\n",
    "print(x_reshaped.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, `view` requires contiguous memory, in contrast to `reshape`. Therefore, `reshape` is safe to use in expanse of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a tensor with dynamic size, you can use `-1` to represent any size. But you can set only one dimension as `-1`. It means that `PyTorch` will calculate the suitable number for that dimension by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate two tensors both row-wise and column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates two tensors\n",
    "x = torch.rand((3, 3))\n",
    "y = torch.rand((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5907, 0.3194, 0.9517],\n",
      "        [0.9803, 0.0220, 0.8626],\n",
      "        [0.2246, 0.1845, 0.1180],\n",
      "        [0.0938, 0.7445, 0.2209],\n",
      "        [0.9670, 0.0245, 0.6205],\n",
      "        [0.5002, 0.7280, 0.6862]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y column-wise\n",
    "print(torch.cat((x, y), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5907, 0.3194, 0.9517, 0.0938, 0.7445, 0.2209],\n",
      "        [0.9803, 0.0220, 0.8626, 0.9670, 0.0245, 0.6205],\n",
      "        [0.2246, 0.1845, 0.1180, 0.5002, 0.7280, 0.6862]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y row-wise\n",
    "print(torch.cat((x, y), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide a `tensor` into multiple chunks using the `chunk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5907, 0.3194, 0.9517]])\n",
      "tensor([[0.9803, 0.0220, 0.8626]])\n",
      "tensor([[0.2246, 0.1845, 0.1180]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.chunk(x, 3)\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might need splits of different sizes. We can use the `split` function instead and define the size of each splits manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5907, 0.3194, 0.9517],\n",
      "        [0.9803, 0.0220, 0.8626]])\n",
      "tensor([[0.2246, 0.1845, 0.1180]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.split(x, split_size_or_sections=[2, 1])\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5907, 0.3194, 0.9517, 0.9803, 0.0220, 0.8626, 0.2246, 0.1845, 0.1180])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrolling all elements\n",
    "x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "torch.Size([64, 196608])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "# converting 2d color images into a vector of pixels\n",
    "print(img.reshape(64, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "# swapping dimensions, changing colour channel dimension to the last\n",
    "print(img.permute(0, 2, 3, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `squeeze` function removes the given dimension, whereas the `unsqueeze` function adds an extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 1])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# add and remove a single dimension to the existing one\n",
    "x = torch.arange(10)\n",
    "print(x)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "print(x.unsqueeze(2).shape)\n",
    "print(x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classification\n",
    "In this short exercise, we will be building a naive **baseline** classifier to classify digits $3$ and $7$. We will utilize the `fast.ai` repository's MNIST SAMPLE dataset that only contains digits $3$ and $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads MNIST dataset from Fast.ai repository\n",
    "# This trimmed dataset contains only 3s and 7s\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast.ai API returns the path to the downloaded dataset. The path is not just a string, rather it is a python Path object Learn more from [**documentation**](https://docs.python.org/3/library/pathlib.html). Or watch this [**tutorial**](https://www.youtube.com/watch?v=YwhOUyTxXVE) for a quick introduction. The new path object comes with many unix commands baked into itself for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/valid'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/labels.csv'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train')]\n"
     ]
    }
   ],
   "source": [
    "print(path.ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `/` operator is one of the most convenient shorthands for creating new paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/7'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/3')]\n"
     ]
    }
   ],
   "source": [
    "print((path / 'train').ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/3/10.png'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/3/10000.png'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/3/10011.png')]\n",
      "\n",
      "[Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/7/10002.png'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/7/1001.png'), Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/7/10014.png')]\n"
     ]
    }
   ],
   "source": [
    "threes = (path/'train/3').ls().sorted()\n",
    "sevens = (path/'train/7').ls().sorted()\n",
    "\n",
    "print(threes[:3])\n",
    "print()\n",
    "print(sevens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6131\n",
      "6265\n"
     ]
    }
   ],
   "source": [
    "print(len(threes))\n",
    "print(len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "\n",
    "len(three_tensors), len(seven_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images((three_tensors[15], seven_tensors[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes = torch.stack(three_tensors).float() / 255\n",
    "\n",
    "print(stacked_threes.shape)\n",
    "print(stacked_sevens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to find the mean representation of $3$ and $7$. Then for any new digit,\n",
    "we will compute the distance from the digit to both $3$ and $7$. We will predict the\n",
    "digit with the smallest distance between the two. Next, we will calculate the average\n",
    "representation of the digits $3$ and $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create some functions to calculate distance between two images, and\n",
    "also to predict digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance(img1: tensor, img2: tensor) -> tensor:\n",
    "    \"\"\"Function to calculate average l1 distance or the mean absolute error between\n",
    "    two images.\"\"\"\n",
    "    return (img1 - img2).abs().mean((-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img: tensor) -> int:\n",
    "    \"\"\"Returns 3 if the distance between given image and average image of the\n",
    "    digit 3 is smaller than that of digit 7. Returns 7 otherwise.\"\"\"\n",
    "    mask = l1_distance(img, mean3) < l1_distance(img, mean7)\n",
    "    mask = torch.where(mask, torch.tensor(3), torch.tensor(7))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "a_7 = stacked_sevens[12]\n",
    "\n",
    "show_images((a_3, a_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(predict(a_3))\n",
    "print(predict(a_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model worked well on the training data. Let's load the validation data and \n",
    "measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float() / 255\n",
    "\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]) \n",
    "valid_7_tens = valid_7_tens.float() / 255\n",
    "\n",
    "valid_3_tens.shape, valid_7_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will combine the $3$ and $7$ tensors to a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 28, 28])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_x = torch.cat([valid_3_tens, valid_7_tens])\n",
    "valid_data_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a tensor to store the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_y = torch.tensor([3] * valid_3_tens.shape[0] + [7] * valid_7_tens.shape[0])\n",
    "valid_data_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will predict and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(valid_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.14229583740234"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = torch.sum(predictions == valid_data_y) / len(predictions) * 100\n",
    "acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "A tensor can be created with `requires_grad=True` so that `torch.autograd` records operations on them for automatic differentiation. For example, we know that\n",
    "$$y = x^2$$\n",
    "$$\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x$$\n",
    "$$\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)=4$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dervative at x = 2:  tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same when inputs have multiple data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  3.,  4., -2., -3., -4.], requires_grad=True)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2, 3, 4, -2, -3, -4], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x ** 2).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dervative at x = 2:  tensor([ 4.,  6.,  8., -4., -6., -8.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
