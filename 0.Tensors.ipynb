{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "A tensor is essentially a matrix that can have multiple discrete dimensions. For example, a 2D tensor can represent a grayscale or black and white image. Similarly, a 3D tensor can represent a multi-channel color image. Lastly, a 4D tensor can represent a sequence of images.\n",
    "\n",
    "Among many of the features that PyTorch tensors provide, some important ones are as follows:\n",
    "- Efficient computation on both CPU and GPU (*NumPy doesn't support GPU*)\n",
    "- Automatic differentiation (*NumPy doesn't have this capability*)\n",
    "- Efficient data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# fastai is a higher level library for PyTorch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check PyTorch Version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can run both in CPU and GPU. `torch.cuda.is_available()` is a convenient function to check if your environment supports GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# setting device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Tensor` can be created from a Python list, with a designated data type, and on a specified device. The `requires_grad` tells whether to compute gradients for any operation with this variable. We will explore this autograd functionality later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float64, device=device, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instances of a `Tensor` object has the following usefult properties.\n",
    "- `ndim` returns the number of *axes* or *dimensions* of the tensor. It is also called *rank*. For example, the above tensor has rank of 1 since it has only 1 axis. In contrast, a $2\\times2$ tensor will have a rank of 2 since it has 2 axes now.\n",
    "- `shape` returns the length of each axis or rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.float64\n",
      "torch.Size([5])\n",
      "1\n",
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(x.numel())\n",
    "print(x.dtype)\n",
    "print(x.shape)\n",
    "print(x.ndim)\n",
    "print(x.requires_grad)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor()` always copies data. If you have a numpy array and want to avoid copying, use `torch.as_tensor()`. Finally, remember that the elements in the list that will be converted to tensor must have the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(10)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.as_tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of y is not copied to z. rather they are using the ame memory address. So, any change in z will be reflected in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11030998544\n",
      "11031087920\n"
     ]
    }
   ],
   "source": [
    "print(id(y))\n",
    "print(id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `from_numpy` function to convert a `ndarray` into a `tensor`. However, this function copies the element of the `ndarray` to create a new `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.from_numpy(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident below, both `y` and `z` have different memory addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11030998544\n",
      "11031085840\n"
     ]
    }
   ],
   "source": [
    "print(id(y))\n",
    "print(id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert a `tensor` back to a `ndarray` using the `numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a `tensor` from a `Pandas` `Series` data structure. We can simply convert a `Series` into a `ndarray` using the `values` property. Then we can simply use the `as_tensor` or `from_numpy` function to create a new `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "January     1\n",
       "February    2\n",
       "March       3\n",
       "April       4\n",
       "May         5\n",
       "June        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series([1, 2, 3, 4, 5, 6],\n",
    "                    index=['January', 'February', 'March', 'April', 'May', 'June'])\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(series.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several useful shorthands for quickly creating tensors of arbitrary shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty((3, 3))\n",
    "y = torch.zeros((2, 2))\n",
    "z = torch.ones((3, 3))\n",
    "p = torch.rand((2, 2))\n",
    "q = torch.eye(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.8091e-45, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further methods for creating `tensor`s. A useful function for plotting mathematical functions is <code>torch.linspace()</code>. <code>torch.linspace()</code> returns evenly spaced numbers over a specified interval. You specify the starting point of the sequence and the ending point of the sequence. The parameter <code>steps</code> indicates the number of samples to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(start=10, end=20, step=5)\n",
    "y = torch.linspace(start=10, end=20, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 15])\n",
      "tensor([10.0000, 12.5000, 15.0000, 17.5000, 20.0000])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `tolist` function to convert a `tensor` directly to a `Python` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 12.5, 15.0, 17.5, 20.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting\n",
    "Sometimes you might need to change the data type of a tensor that you have already created. There are two different ways to that in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.bool())\n",
    "# convert to int16\n",
    "print(x.short())\n",
    "# convert to int64\n",
    "print(x.long())\n",
    "# convert to float16\n",
    "print(x.half())\n",
    "# convert to float32\n",
    "print(x.float())\n",
    "# convert to float64\n",
    "print(x.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to typecast using the `to` function, and giving the intended `torch` type as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.to(torch.bool))\n",
    "# convert to int16\n",
    "print(x.to(torch.int16))\n",
    "# convert to int64\n",
    "print(x.to(torch.int64))\n",
    "# convert to float16\n",
    "print(x.to(torch.float16))\n",
    "# convert to float32\n",
    "print(x.to(torch.float32))\n",
    "# convert to float64\n",
    "print(x.to(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([-3, -3, -3])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# addition\n",
    "z = x + y\n",
    "print(z)\n",
    "# subtraction\n",
    "z = x - y\n",
    "print(z)\n",
    "# element-wise division\n",
    "z = x / y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# inplace operations, more efficient\n",
    "x.add_(y)\n",
    "print(x)\n",
    "x.subtract_(y)\n",
    "print(x)\n",
    "x.pow_(2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3312, 0.1365, 0.4454, 0.4245, 0.3576],\n",
       "        [0.4325, 0.1696, 0.6014, 0.7058, 0.5788],\n",
       "        [0.1397, 0.0560, 0.1915, 0.2070, 0.1715],\n",
       "        [0.5276, 0.2258, 0.6899, 0.5268, 0.4594]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "x = torch.rand((4, 2))\n",
    "y = torch.rand((2, 5))\n",
    "x.mm(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 30])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch matrix multiplication\n",
    "batch = 64\n",
    "x = torch.rand((batch, 10, 20))\n",
    "y = torch.rand((batch, 20, 30))\n",
    "x.bmm(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 14, 24, 36])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "x = torch.arange(5)\n",
    "y = torch.arange(5, 10)\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9772, 0.5950, 0.1627, 0.8569, 0.1666])\n",
      "tensor([0.5370, 0.2950, 0.2177, 0.9516, 0.8051])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5))\n",
    "y = torch.rand((5))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True,  True,  True])\n",
      "tensor([ True,  True, False, False, False])\n",
      "tensor([False, False, False, False, False])\n",
      "tensor([True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(x < y)\n",
    "print(x > y)\n",
    "print(x == y)\n",
    "print(x != y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "PyTorch, when it tries to perform a simple operation between two tensors of different ranks, will use broadcasting: it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5, 5))\n",
    "y = torch.rand((5))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6257, 1.6323, 0.4242, 1.2971, 0.7699],\n",
       "        [1.3151, 1.7536, 0.4185, 0.7434, 0.8552],\n",
       "        [0.8354, 1.8157, 0.4135, 1.2767, 0.7266],\n",
       "        [1.5326, 1.3267, 0.4649, 1.3536, 0.6264],\n",
       "        [1.0648, 1.6381, 0.9111, 0.5647, 0.4938]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (5) vector is broadcasted to each row of (5, 5) matrix\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions\n",
    "**It is often better to avoid loops and use these functions to speed up performance.\n",
    "Raw Python loops are too slow. To levelrage the underlying `C` codes of `PyTorch`,\n",
    "avoid them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  9, 11, 13, 15])\n",
      "tensor([15, 40])\n"
     ]
    }
   ],
   "source": [
    "# row-wise sum\n",
    "print(x.sum(dim=0))\n",
    "# column-wise sum\n",
    "print(x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([1, 2, 3, 4, 5]),\n",
      "indices=tensor([0, 0, 0, 0, 0]))\n",
      "torch.return_types.min(\n",
      "values=tensor([1, 6]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# returns the minimum element of each column\n",
    "print(x.min(dim=0))\n",
    "# returns the minimum element of each row\n",
    "print(x.min(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([ 5, 10]),\n",
      "indices=tensor([4, 4]))\n",
      "torch.return_types.max(\n",
      "values=tensor([ 6,  7,  8,  9, 10]),\n",
      "indices=tensor([1, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# returns the maximum element of each row\n",
    "print(x.max(dim=1))\n",
    "# returns the maximum element of each column\n",
    "print(x.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4])\n",
      "tensor([1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the maximum element row-wise\n",
    "print(x.argmax(dim=1))\n",
    "# returns the position of the maximum element column-wise\n",
    "print(x.argmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0])\n",
      "tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the minimum element row-wise\n",
    "print(x.argmin(dim=1))\n",
    "# returns the position of the minimum element column-wise\n",
    "print(x.argmin(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 4.5000, 5.5000, 6.5000, 7.5000])\n"
     ]
    }
   ],
   "source": [
    "# returns means of each column\n",
    "print(torch.mean(x.float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8358, 0.6128, 0.5464, 0.5177, 0.4006, 0.8595, 0.2879, 0.7886, 0.7109,\n",
      "        0.4455])\n",
      "tensor([0.2879, 0.4006, 0.4455, 0.5177, 0.5464, 0.6128, 0.7109, 0.7886, 0.8358,\n",
      "        0.8595])\n",
      "tensor([6, 4, 9, 3, 2, 1, 8, 7, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "# sorts tensor\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x, indices = x.sort(descending=False)\n",
    "print(x)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  4,  5,  6,  7, 10, 11])\n",
      "tensor([ 2,  2,  4,  5,  6,  7, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# clamp tensor between values\n",
    "x = torch.tensor([0, 1, 4, 5, 6, 7, 10, 11])\n",
    "print(x)\n",
    "# any element less than 2 is set to 2, and any element more than 10 is set to 10\n",
    "x = x.clamp(min=2, max=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 0, 1, 1, 1], dtype=torch.bool)\n",
    "# checks if any of the values is true\n",
    "print(x.any())\n",
    "# checks if all the values are true\n",
    "print(x.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "The contents of a tensor can be accessed and modified using Pythonâ€™s indexing and slicing notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "feature_size = (3, 256, 256)\n",
    "\n",
    "img = torch.rand((batch_size, *feature_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# selecting first image\n",
    "print(img[0].shape)\n",
    "# selecting first color channel of first image\n",
    "print(img[0, 0].shape)\n",
    "# selecting first row of first color channel of first image\n",
    "print(img[0, 0, 0].shape)\n",
    "# slecting first column of first color channel of first image\n",
    "print(img[0, 0, :, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number on the left side of the colon represents the index of the first value. The number on the right side of the colon is always 1 larger than the index of the last value. For example, <code>tensor_sample\\[1:4]</code> means you get values from the index 1 to index 3 <i>(4-1)</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# selecting 3rd column of 2nd color channel of all images\n",
    "print(img[:, 1, :, 2].shape)\n",
    "# selecting 3rd column of 2nd color channel of first 10 images\n",
    "print(img[:10, 1, :, 2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage `Fancy Indexing` and use boolean conditions to select specific values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True, False, False])\n",
      "tensor([1, 2, 3])\n",
      "tensor([ True, False, False, False,  True])\n",
      "tensor([1, 5])\n",
      "tensor([ True, False, False, False, False])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(x < 4)\n",
    "print(x[x < 4])\n",
    "print((x < 2) | (x > 4))\n",
    "print(x[(x < 2) | (x > 4)])\n",
    "print((x < 3) & (x < 2))\n",
    "print(x[(x < 3) & (x < 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `where` function returns elements as it is if condition is met, otherwise changes value according to given formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1, 102, 103, 104, 105])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.where(x < 2, x + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique` function returns the unique elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.Tensor.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a single item\n",
    "x[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can select a specific value or a range of values from a `tensor`, we can also assign new values to those positions. For example, below we have selected the first 2 items of `x` and assigned new values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 101,   3,   4,   5])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2] = torch.tensor([100, 101])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping & Resizing\n",
    "For some operations, the input tensors need to have a certain number of dimensions (also called rank) and a certain number of elements (shape). So, we might have to change the shape of a tensor, add a new dimension, or make a dimension smaller that isn't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `view` function can be used to reshape a vector. For example, before `x` was a 1 dimensional vector. Later we reshaped it into a 2 dimensional matrix with 3 rows and 4 columns. The number of elements in a tensor must remain constant after applying view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.view(3, 4)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x_reshaped.shape)\n",
    "print(x_reshaped.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, `view` requires contiguous memory, in contrast to `reshape`. Therefore, `reshape` is safe to use in expanse of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a tensor with dynamic size, you can use `-1` to represent any size. But you can set only one dimension as `-1`. It means that `PyTorch` will calculate the suitable number for that dimension by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate two tensors both row-wise and column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates two tensors\n",
    "x = torch.rand((3, 3))\n",
    "y = torch.rand((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5869, 0.1051, 0.5101],\n",
      "        [0.2798, 0.0544, 0.2841],\n",
      "        [0.4957, 0.2195, 0.5552],\n",
      "        [0.4691, 0.1821, 0.1135],\n",
      "        [0.6151, 0.1241, 0.2011],\n",
      "        [0.4262, 0.3634, 0.9291]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y column-wise\n",
    "print(torch.cat((x, y), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5869, 0.1051, 0.5101, 0.4691, 0.1821, 0.1135],\n",
      "        [0.2798, 0.0544, 0.2841, 0.6151, 0.1241, 0.2011],\n",
      "        [0.4957, 0.2195, 0.5552, 0.4262, 0.3634, 0.9291]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y row-wise\n",
    "print(torch.cat((x, y), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide a `tensor` into multiple chunks using the `chunk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5869, 0.1051, 0.5101]])\n",
      "tensor([[0.2798, 0.0544, 0.2841]])\n",
      "tensor([[0.4957, 0.2195, 0.5552]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.chunk(x, 3)\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might need splits of different sizes. We can use the `split` function instead and define the size of each splits manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5869, 0.1051, 0.5101],\n",
      "        [0.2798, 0.0544, 0.2841]])\n",
      "tensor([[0.4957, 0.2195, 0.5552]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.split(x, split_size_or_sections=[2, 1])\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6183, 0.7554, 0.0660, 0.3500, 0.0433, 0.4838, 0.6038, 0.5606, 0.1471])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrolling all elements\n",
    "x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "torch.Size([64, 196608])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "# converting 2d color images into a vector of pixels\n",
    "print(img.reshape(64, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "# swapping dimensions, changing colour channel dimension to the last\n",
    "print(img.permute(0, 2, 3, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `squeeze` function removes the given dimension, whereas the `unsqueeze` function adds an extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 1])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# add and remove a single dimension to the existing one\n",
    "x = torch.arange(10)\n",
    "print(x)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "print(x.unsqueeze(2).shape)\n",
    "print(x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3s Vs. 7s\n",
    "In this short exercise, we will be building a naive **baseline** classifier to classify digits $3$ and $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.14% [3219456/3214948 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads MNIST dataset from Fast.ai repository\n",
    "# This trimmed dataset contains only 3s and 7s\n",
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/valid'),Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/labels.csv'),Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/7'),Path('/Users/musabbirhasansammak/.fastai/data/mnist_sample/train/3')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/'train/3').ls().sorted()\n",
    "sevens = (path/'train/7').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "len(three_tensors), len(seven_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(three_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes = torch.stack(three_tensors).float() / 255\n",
    "print(stacked_threes.shape)\n",
    "print(stacked_sevens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "show_image(a_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.2021))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_3_abs = (a_3 - mean3).abs().mean()\n",
    "dist_3_sqr = ((a_3 - mean3) ** 2).mean().sqrt()\n",
    "dist_3_abs, dist_3_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_7_abs = (a_3 - mean7).abs().mean()\n",
    "dist_7_sqr = ((a_3 - mean7) ** 2).mean().sqrt()\n",
    "dist_7_abs, dist_7_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float() / 255\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]) \n",
    "valid_7_tens = valid_7_tens.float() / 255\n",
    "\n",
    "valid_3_tens.shape, valid_7_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1010]) torch.Size([1010])\n"
     ]
    }
   ],
   "source": [
    "valid_3_dist_from_3 = (valid_3_tens - mean3).abs().mean((-1, -2))\n",
    "valid_3_dist_from_7 = (valid_3_tens - mean7).abs().mean((-1, -2))\n",
    "\n",
    "print(valid_3_dist_from_3.shape, valid_3_dist_from_7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1028]) torch.Size([1028])\n"
     ]
    }
   ],
   "source": [
    "valid_7_dist_from_3 = (valid_7_tens - mean3).abs().mean((-1, -2))\n",
    "valid_7_dist_from_7 = (valid_7_tens - mean7).abs().mean((-1, -2))\n",
    "\n",
    "print(valid_7_dist_from_3.shape, valid_7_dist_from_7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_three = valid_3_dist_from_3 < valid_3_dist_from_7\n",
    "is_seven = valid_7_dist_from_7 < valid_7_dist_from_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(926)\n",
      "tensor(1013)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(is_three))\n",
    "print(torch.sum(is_seven))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  tensor(95.1423)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', (torch.sum(is_three) + torch.sum(is_seven)) / (is_three.numel() + is_seven.numel()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "A tensor can be created with `requires_grad=True` so that `torch.autograd` records operations on them for automatic differentiation. For example, we know that\n",
    "$$y = x^2$$\n",
    "$$\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x$$\n",
    "$$\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)=4$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dervative at x = 2:  tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
