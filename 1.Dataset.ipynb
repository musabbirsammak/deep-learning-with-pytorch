{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly utilize a tensor for training if the training dataset is short and can be loaded into memory. In common usage cases, when the dataset is too huge to fit in computer memory, we must load the data from the primary storage device batch by batch. To prevent overfitting, we may need to build a data-processing pipeline to perform transformations and preprocessing processes to our data, such as mean centering, scaling, or introducing noise.\n",
    "\n",
    "Manually applying preprocessing functions is tedious. PyTorch has a class for building efficient preprocessing pipelines. In this section, we'll explore how to create a PyTorch `Dataset` and `DataLoader` and load, shuffle, and batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides `torch.utils.data.Dataset` that allow you to use your own data. Dataset stores the samples and their corresponding labels to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom Dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`.\n",
    "- The `__init__` function is run once when instantiating the Dataset object.\n",
    "- The `__len__` function returns the number of samples in our dataset.\n",
    "- The `__getitem__` function loads and returns a sample from the dataset at the given index idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyData(Dataset):\n",
    "    def __init__(self, length = 100, transform = None):\n",
    "        self.len = length\n",
    "        self.x = 2 * torch.ones(length, 2)\n",
    "        self.y = torch.ones(length, 1)\n",
    "        self.transform = transform\n",
    "     \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)     \n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to customize the indexing and length method by customizing <code>def \\__getitem\\_\\_(self, index)</code> and <code>def \\__len\\_\\_(self)</code>. As a result, we can apply the same indexing convention as a <code>list</code>, and apply the fuction <code>len</code> on the <code>ToyData</code> object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ToyData()\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = dataset[i]\n",
    "    print(\"Index: \", i, '; x:', x, '; y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object is an Iterable; as a result, we  apply the loop directly on the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset:\n",
    "    print(' x:', x, 'y:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training. You can also create a class for transforming the data. In this case, we will try to add 1 to our features and multiply our labels by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransforms(object):\n",
    "    def __init__(self, addx = 1, muly = 2):\n",
    "        self.addx = addx\n",
    "        self.muly = muly\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x = sample[0]\n",
    "        y = sample[1]\n",
    "        x = x + self.addx\n",
    "        y = y * self.muly\n",
    "        sample = x, y\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the transform object every time we create a new <code>ToyData</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = CustomTransforms()\n",
    "dataset = ToyData(transform=transform)\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = dataset[i]\n",
    "    print(\"Index: \", i, '; x:', x, '; y:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose\n",
    "`Compose` composes several transforms together. For example, we have splitted the previous transformation into two separate classes. The `FeatureTransformer` adds `1` to all the features. On the other hand the `LabelTransformer` multiplies the labels by `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransforms(object):\n",
    "    def __init__(self, addx = 1):\n",
    "        self.addx = addx\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x = sample[0]\n",
    "        y = sample[1]\n",
    "        x = x + self.addx\n",
    "        sample = x, y\n",
    "        return sample\n",
    "\n",
    "\n",
    "class LabelTransforms(object):\n",
    "    def __init__(self, muly = 2):\n",
    "        self.muly = muly\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x = sample[0]\n",
    "        y = sample[1]\n",
    "        y = y * self.muly\n",
    "        sample = x, y\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([FeatureTransforms(), LabelTransforms()])\n",
    "dataset = ToyData(transform=transform)\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = dataset[i]\n",
    "    print(\"Index: \", i, '; x:', x, '; y:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval. DataLoader is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=dataset, batch_size=1)\n",
    "\n",
    "for data in trainloader:\n",
    "    print('First data:', data[0])\n",
    "    print('First label:', data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "for data in trainloader:\n",
    "    print('Mini-batch feature shape:', data[0].shape)\n",
    "    print('Mini-batch label shape:', data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it Altogether\n",
    "\n",
    "Now we will create our custom `Dataset` object to load our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"datasets/mnist-fashion-small\"\n",
    "metadata_name ='metadata.csv'\n",
    "img_dir = 'img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = os.path.join(directory, metadata_name)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata = metadata.iloc[:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = metadata.iloc[1, 1]\n",
    "image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(directory, image_name)\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)\n",
    "plt.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.title(metadata.iloc[1, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, metadata_name, data_dir, transform=None, label_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "        self.metadata = pd.read_csv(os.path.join(data_dir, metadata_name))\n",
    "        self.len = self.metadata.shape[0] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_dir, self.metadata.iloc[idx, 1])\n",
    "        image = Image.open(img_name)    \n",
    "        y = metadata.iloc[idx, 0]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.label_transform:\n",
    "            y = self.label_transform(y)\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(metadata_name=metadata_name, data_dir=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0][0], cmap='gray')\n",
    "plt.title(dataset[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torchvision.transforms module offers several commonly-used transforms out of the box. For example, The features are in PIL Image format. For training, we need the features as normalized tensors. To make these transformations, we use the built-in `ToTensor` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\n",
    "dataset = Dataset(metadata_name=metadata_name, data_dir=directory,\n",
    "                  transform=croptensor_data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0][0].squeeze(0), cmap='gray')\n",
    "plt.title(dataset[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset, shuffle=True, batch_size=20)\n",
    "\n",
    "for data in trainloader:\n",
    "    print('Mini-batch feature shape:', data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_crop = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "])\n",
    "\n",
    "color_jitter = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.05, saturation=0.5, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "])\n",
    "\n",
    "random_rotation = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "])\n",
    "\n",
    "random_flip = transformations = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "])\n",
    "\n",
    "random_grayscale = transforms.Compose([\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "])\n",
    "\n",
    "random_blur = transforms.Compose([\n",
    "    transforms.GaussianBlur((5, 5)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0], std=[1.0])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=transforms.ToTensor())\n",
    "augmented_random_crop = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=random_crop)\n",
    "augmented_color_jitter = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=color_jitter)\n",
    "augmented_random_rotation = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=random_rotation)\n",
    "augmented_random_flip = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=random_flip)\n",
    "augmented_random_grayscale = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=random_grayscale)\n",
    "augmented_random_blur = CustomDataset(metadata_name=metadata_name, data_dir=directory, transform=random_blur)\n",
    "\n",
    "final_dataset = torch.utils.data.ConcatDataset([\n",
    "    dataset,\n",
    "    augmented_random_crop,\n",
    "    augmented_color_jitter,\n",
    "    augmented_random_rotation,\n",
    "    augmented_random_flip,\n",
    "    augmented_random_grayscale,\n",
    "    augmented_random_blur\n",
    "])\n",
    "trainloader = DataLoader(final_dataset, shuffle=True, batch_size=20)\n",
    "\n",
    "for data in trainloader:\n",
    "    print('Mini-batch feature shape:', data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0][0].squeeze(0))\n",
    "plt.title(dataset[0][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first download the Oxford-IIIT Pets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_image_files` from `fastai` library fetches all the image files of the given path recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(path / 'images')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the cat files start with capital letters, whereas that of dogs start with lowercase. So, we will write a function that can correctly label cats and dogs by looking at the filename only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(f):\n",
    "    return f[0].isupper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data in a DataLoaders object before we can use it in a model. Here, we have a labeling function that uses the file names, so we will use ImageDataLoaders.from_name_func. There are other ImageDataLoaders factory methods that might work better for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`item_tfms` is kind of `Transformation` that is applied to all images. Here, all the images will be randomly cropped on the largest dimension first so that the final image can be a square, then resizing it to a $224\\times224$ resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will train a ResNet model for classification, and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_results()` randomly shows some of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Interpretation` object can show where the model made most errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = Interpretation.from_learner(learn)\n",
    "interp.plot_top_losses(9, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extrac the breed name using regular expression. At the same time we will also use data augmentation using the `batch_tfms` parameter. `aug_transforms` suppies some common augementation techniques that are well suited to most datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'^(.*)_\\d+.jpg'\n",
    "dls = ImageDataLoaders.from_name_re(path, files, pattern, item_tfms=Resize(460),\n",
    "                                    batch_tfms=aug_transforms(size=224))\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataBlock API\n",
    "\n",
    "A datablock is built by giving the fastai library a bunch of informations:\n",
    "\n",
    "- The types used, through an argument called `blocks`: here we have images and categories, so we pass `ImageBlock` and `CategoryBlock`.\n",
    "- How to get the raw items, here our function `get_image_files`.\n",
    "- how to label those items, here with the same regular expression as before.\n",
    "- how to split those items, here with a random splitter.\n",
    "- the `item_tfms` and `batch_tfms` like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = DataBlock(blocks=(ImageBlock, CategoryBlock), \n",
    "                 get_items=get_image_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "                 item_tfms=Resize(460),\n",
    "                 batch_tfms=aug_transforms(size=224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pets` data block is empty initially. To actually load the data, we have to call the `dataloaders` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = pets.dataloaders(untar_data(URLs.PETS)/\"images\")\n",
    "dls.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
