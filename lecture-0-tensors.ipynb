{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "A tensor is essentially a matrix that can have multiple discrete dimensions. For example, a 2D tensor can represent a grayscale or black and white image. Similarly, a 3D tensor can represent a multi-channel color image. Lastly, a 4D tensor can represent a sequence of images.\n",
    "\n",
    "Among many of the features that PyTorch tensors provide, some important ones are as follows:\n",
    "- Efficient computation on both CPU and GPU (*NumPy doesn't support GPU*)\n",
    "- Automatic differentiation (*NumPy doesn't have this capability*)\n",
    "- Efficient data Input/Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# fastai is a higher level library for PyTorch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cu118'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check PyTorch Version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can run both in CPU and GPU. `torch.cuda.is_available()` is a convenient function to check if your environment supports GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Tensor` can be created from a Python list, with a designated data type, and on a specified device. The `requires_grad` tells whether to compute gradients for any operation with this variable. We will explore this autograd functionality later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, device=device, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instances of a `Tensor` object has the following usefult properties.\n",
    "- `ndim` returns the number of *axes* or *dimensions* of the tensor. It is also called the *rank* of a tensor. For example, the above tensor has rank of 1 since it has only 1 axis. In contrast, a $5\\times10$ tensor will have a rank of 2 since it has 2 axes (rows and columns).\n",
    "- `shape` returns the length of each axis or rank. For example, a $15\\times10\\times5$ tensor has 3 dimensions or axes, and the shape of each axes are 15, 10, and 5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of elements:  10\n",
      "Data type of each element:  torch.float32\n",
      "Rank/Dimension:  2\n",
      "Length of each dimension:  torch.Size([2, 5])\n",
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Total number of elements: ', x.numel())\n",
    "print('Data type of each element: ', x.dtype)\n",
    "print('Rank/Dimension: ', x.ndim)\n",
    "print('Length of each dimension: ', x.shape)\n",
    "print(x.requires_grad)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tensor()` Vs. `as_tensor()` Vs. `from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.tensor()` always copies data and discards and previous autograd history.\n",
    "- If you do not want to copy data and share autograd history, you may use `torch.as_tensor()`.\n",
    "- `torch.from_numpy()` creates a tensor that shares storage with a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.arange(5, dtype=np.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.from_numpy(y)\n",
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification to `z` will be reflected on `y` and vice-versa since they share memory storage now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[100.75   1.     2.     3.     4.  ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 100.75\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if you use the `tensor` function, the values of y is copied to z and they are not using the same memory address. So, any change in z will not be reflected in y and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([500.7500,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[100.75   1.     2.     3.     4.  ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 500.75\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if your use the `as_tensor` function, the values of y is not copied to z and they are using the ame memory address. So, any change in z will be reflected in y and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100.7500,   1.0000,   2.0000,   3.0000,   4.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.as_tensor(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident below, both `y` and `z` have different memory addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([125.9000,   1.0000,   2.0000,   3.0000,   4.0000])\n",
      "[125.9   1.    2.    3.    4. ]\n"
     ]
    }
   ],
   "source": [
    "z[0] = 125.90\n",
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert a `tensor` back to a `ndarray` using the `numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125.9,   1. ,   2. ,   3. ,   4. ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a `tensor` from a `Pandas` `Series` data structure. We can simply convert a `Series` into a `ndarray` using the `values` property. Then we can simply use the `as_tensor` or `from_numpy` function to create a new `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "January     1\n",
       "February    2\n",
       "March       3\n",
       "April       4\n",
       "May         5\n",
       "June        6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = pd.Series([1, 2, 3, 4, 5, 6],\n",
    "                    index=['January', 'February', 'March', 'April', 'May', 'June'])\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(series.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorthands for Tensor Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several useful shorthands for quickly creating tensors of arbitrary shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty((3, 3))\n",
    "y = torch.zeros((2, 2))\n",
    "z = torch.ones((3, 3))\n",
    "p = torch.rand((3, 7))\n",
    "q = torch.eye(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2306, 0.2598, 0.9197, 0.4072, 0.5646, 0.3599, 0.3451],\n",
      "        [0.0035, 0.2995, 0.7199, 0.0114, 0.1493, 0.0306, 0.5116],\n",
      "        [0.9740, 0.6109, 0.1817, 0.5202, 0.4960, 0.7194, 0.2120]])\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful function for plotting mathematical functions is <code>torch.linspace()</code> and `torch.arange()`. <code>torch.linspace()</code> returns evenly spaced numbers over a specified interval. You specify the starting point of the sequence and the ending point of the sequence. The parameter <code>steps</code> indicates the number of samples to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(start=10, end=20, step=5)\n",
    "y = torch.linspace(start=10, end=20, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 15])\n",
      "tensor([10.0000, 12.5000, 15.0000, 17.5000, 20.0000])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `tolist` function to convert a `tensor` directly to a `Python` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 12.5, 15.0, 17.5, 20.0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting\n",
    "Sometimes you might need to change the data type of a tensor that you have already created. There are two different ways to that in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int16)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.bool())\n",
    "# convert to int16\n",
    "print(x.short())\n",
    "# convert to int64\n",
    "print(x.long())\n",
    "# convert to float16\n",
    "print(x.half())\n",
    "# convert to float32\n",
    "print(x.float())\n",
    "# convert to float64\n",
    "print(x.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to typecast using the `to` function, and giving the intended `torch` type as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True,  True,  True])\n",
      "tensor([0, 1, 2, 3], dtype=torch.int8)\n",
      "tensor([0, 1, 2, 3])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float16)\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([0., 1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert to boolean\n",
    "print(x.to(torch.bool))\n",
    "# convert to int16\n",
    "print(x.to(torch.int8))\n",
    "# convert to int64\n",
    "print(x.to(torch.int64))\n",
    "# convert to float16\n",
    "print(x.to(torch.float16))\n",
    "# convert to float32\n",
    "print(x.to(torch.float32))\n",
    "# convert to float64\n",
    "print(x.to(torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 7, 9])\n",
      "tensor([-3, -3, -3])\n",
      "tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# addition\n",
    "z = x + y\n",
    "print(z)\n",
    "# subtraction\n",
    "z = x - y\n",
    "print(z)\n",
    "# element-wise division\n",
    "z = x / y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11, 117, 573])\n",
      "tensor([  7, 112, 567])\n",
      "tensor([    49,  12544, 321489])\n",
      "tensor([    343,   87808, 2250423])\n"
     ]
    }
   ],
   "source": [
    "# inplace operations, more efficient\n",
    "x.add_(y)\n",
    "print(x)\n",
    "x.subtract_(y)\n",
    "print(x)\n",
    "x.pow_(2)\n",
    "print(x)\n",
    "x.multiply_(7)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1763, 0.0400, 0.0360, 0.1831, 0.0518],\n",
       "        [0.8177, 0.1403, 0.1278, 0.8529, 0.2958],\n",
       "        [0.3159, 0.0863, 0.0770, 0.3271, 0.0751],\n",
       "        [0.7094, 0.1731, 0.1551, 0.7361, 0.1939]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "x = torch.rand((4, 2))\n",
    "y = torch.rand((2, 5))\n",
    "x.mm(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 30])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch matrix multiplication\n",
    "batch = 64\n",
    "x = torch.rand((batch, 10, 20))\n",
    "y = torch.rand((batch, 20, 30))\n",
    "x.bmm(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 14, 24, 36])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "x = torch.arange(5)\n",
    "y = torch.arange(5, 10)\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(80)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Operataions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0243, 0.0772, 0.1427, 0.2622, 0.3704])\n",
      "tensor([0.3154, 0.2643, 0.9942, 0.9226, 0.7457])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5))\n",
    "y = torch.rand((5))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True])\n",
      "tensor([False, False, False, False, False])\n",
      "tensor([False, False, False, False, False])\n",
      "tensor([True, True, True, True, True])\n",
      "tensor([False, False, False, False, False])\n",
      "tensor([True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "print(x < y)\n",
    "print(x > y)\n",
    "print(x == y)\n",
    "print(x != y)\n",
    "print(x >= y)\n",
    "print(x <= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((x != y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "PyTorch, when it tries to perform a simple operation between two tensors of different ranks, will use broadcasting: it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. For example, you can not add a $[3\\times 3]$ matrix with a $[3\\times 4]$ matrix beacuse the later has an extra column and there is nothing left to add with it after you have added the first $3$ with each other. However, if you add a $[3\\times 3]$ matrix with a $[1\\times 3]$ vector, you can add the vector with each row of the matrix. Here, the $[1\\times 3]$ vector, which has smaller rank, is expanded to $[3\\times 3]$ matrix by repeating the $[1\\times 3]$ vector $3$ times. Consider the following image.\n",
    "\n",
    "![Broadcasting](./images/broadcasting.png \"Broadcasting\")\n",
    "\n",
    "<small>This image was taken from **Python Data Science Handbook: Essential Tools for Working with Data 1st Edition by Jake VanderPlas**.</small>\n",
    "\n",
    "### Boradcasting with a Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [3, 4, 5],\n",
    "    [5, 6, 7]\n",
    "])\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the shape of $t$ is $[3\\times 3]$. Now, if we substract $5$, which is a scalar, from $t$, PyTorch will consider a matrix of $5$ of the same shape $[3\\times 3]$ as $t$ like the following\n",
    "$$\n",
    "[\n",
    "    [5, 5, 5],\\\\\n",
    "    [5, 5, 5],\\\\\n",
    "    [5, 5, 5]\n",
    "]\n",
    "$$\n",
    "and substract it from $t$ using the normal elementwise substraction. **However, PyTorch will never actually create this matrix so that it can save memory.** As you can see below, $5$ is substracted from each element of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t - 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting with a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([[1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [5, 6, 7]])\n",
      "Number of dimensions:  2\n",
      "Length of each dimension:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print('Tensor: ', t)\n",
    "print('Number of dimensions: ', t.ndim)\n",
    "print('Length of each dimension: ', t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:  tensor([1, 2, 3])\n",
      "Number of dimensions:  1\n",
      "Length of each dimension:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1, 2, 3])\n",
    "print('Vector: ', v)\n",
    "print('Number of dimensions: ', v.ndim)\n",
    "print('Length of each dimension: ', v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add the vector $v$ to the matrix $t$, the vector will be expanded to match the dimension of the matrix $t$. The vector will act like a matrix of the same shape of $t$ like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.expand_as(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add it with matrix $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 4,  6,  8],\n",
       "        [ 6,  8, 10]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + v.expand_as(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boradcasting does this expansion automatically whenever you are performing any mathematical operation between tensors that have different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 4,  6,  8],\n",
       "        [ 6,  8, 10]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Rule\n",
    "When operating on two tensors, PyTorch compares their shapes elementwise. It starts with the trailing dimensions and works its way backward, adding $1$ when it meets empty dimensions. Two dimensions are compatible when one of the following is true:\n",
    "- They are equal.\n",
    "- One of them is $1$, in which case that dimension is broadcast to make it the same as the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x:  4\n",
      "Length of each dimension of x:  torch.Size([64, 3, 256, 256])\n",
      "\n",
      "Dimension of y:  3\n",
      "Length of each dimension of y:  torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((64, 3, 256, 256))\n",
    "y = torch.rand((3, 256, 256))\n",
    "\n",
    "print('Dimension of x: ', x.ndim)\n",
    "print('Length of each dimension of x: ', x.shape)\n",
    "print()\n",
    "print('Dimension of y: ', y.ndim)\n",
    "print('Length of each dimension of y: ', y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start comparing the lengths of each dimension from the end. In the last dimension, both $x$ and $y$ have $256$. The same is true for the $2^{nd}$ from the last dimension. After that, $x$ has extra $2$ dimensions which $y$ does not have, but we can simply add two extra dimensions in $y$ using the `unsqueeze` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 256, 256])\n",
      "torch.Size([1, 1, 1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "y = y.unsqueeze(0)\n",
    "print(y.shape)\n",
    "y = y.unsqueeze(0)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the lengths of each dimension in both $x$ and $y$ are either same or one of them is $1$. So, our rule holds. We can perform any mathematical operation between them as we like. **However, PyTorch does these `unsqueeze` operations automatically.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions\n",
    "**It is often better to avoid loops and use these functions to speed up performance.\n",
    "Raw Python loops are too slow. To levelrage the underlying `C` codes of `PyTorch`,\n",
    "avoid them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10]\n",
    "])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  9, 11, 13, 15])\n",
      "tensor([15, 40])\n"
     ]
    }
   ],
   "source": [
    "# row-wise sum\n",
    "print(x.sum(dim=0))\n",
    "# column-wise sum\n",
    "print(x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.min(\n",
      "values=tensor([1, 2, 3, 4, 5]),\n",
      "indices=tensor([0, 0, 0, 0, 0]))\n",
      "\n",
      "torch.return_types.min(\n",
      "values=tensor([1, 6]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# returns the minimum element of each column\n",
    "print(x.min(dim=0))\n",
    "print()\n",
    "# returns the minimum element of each row\n",
    "print(x.min(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([ 5, 10]),\n",
      "indices=tensor([4, 4]))\n",
      "\n",
      "torch.return_types.max(\n",
      "values=tensor([ 6,  7,  8,  9, 10]),\n",
      "indices=tensor([1, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# returns the maximum element of each row\n",
    "print(x.max(dim=1))\n",
    "print()\n",
    "# returns the maximum element of each column\n",
    "print(x.max(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1])\n",
      "tensor([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the maximum element row-wise\n",
    "print(x.argmax(dim=0))\n",
    "# returns the position of the maximum element column-wise\n",
    "print(x.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# returns the position of the minimum element row-wise\n",
    "print(x.argmin(dim=0))\n",
    "# returns the position of the minimum element column-wise\n",
    "print(x.argmin(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5000, 4.5000, 5.5000, 6.5000, 7.5000])\n"
     ]
    }
   ],
   "source": [
    "# returns means of each column\n",
    "print(torch.mean(x.float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5936, 0.6652, 0.7727, 0.9042, 0.6373, 0.0824, 0.0247, 0.6230, 0.8667,\n",
      "        0.1418])\n",
      "tensor([0.0247, 0.0824, 0.1418, 0.5936, 0.6230, 0.6373, 0.6652, 0.7727, 0.8667,\n",
      "        0.9042])\n",
      "tensor([6, 5, 9, 0, 7, 4, 1, 2, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "# sorts tensor\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x, indices = x.sort(descending=False)\n",
    "print(x)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  4,  5,  6,  7, 10, 11])\n",
      "tensor([ 2,  2,  4,  5,  6,  7, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# clamp tensor between values\n",
    "x = torch.tensor([0, 1, 4, 5, 6, 7, 10, 11])\n",
    "print(x)\n",
    "# any element less than 2 is set to 2, and any element more than 10 is set to 10\n",
    "x = x.clamp(min=2, max=10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 0, 1, 1, 1], dtype=torch.bool)\n",
    "# checks if any of the values is true\n",
    "print(x.any())\n",
    "# checks if all the values are true\n",
    "print(x.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "The contents of a tensor can be accessed and modified using Pythonâ€™s indexing and slicing notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "feature_size = (3, 256, 256)\n",
    "\n",
    "img = torch.rand((batch_size, *feature_size))\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# selecting first image\n",
    "print(img[0].shape)\n",
    "# selecting first color channel of first image\n",
    "print(img[0, 0].shape)\n",
    "# selecting first row of first color channel of first image\n",
    "print(img[0, 0, 0].shape)\n",
    "# slecting first column of first color channel of first image\n",
    "print(img[0, 0, :, 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number on the left side of the colon represents the index of the first value. The number on the right side of the colon is always 1 larger than the index of the last value. For example, <code>tensor_sample\\[1:4]</code> means you get values from the index 1 to index 3 <i>(4-1)</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# selecting 3rd column of 2nd color channel of all images\n",
    "print(img[:, 1, :, 2].shape)\n",
    "# selecting 3rd column of 2nd color channel of first 10 images\n",
    "print(img[:10, 1, :, 2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage fancy indexing and use boolean conditions to select specific values too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True, False, False])\n",
      "tensor([1, 2, 3])\n",
      "tensor([ True, False, False, False,  True])\n",
      "tensor([1, 5])\n",
      "tensor([ True, False, False, False, False])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "print(x < 4)\n",
    "print(x[x < 4])\n",
    "\n",
    "print((x < 2) | (x > 4))\n",
    "print(x[(x < 2) | (x > 4)])\n",
    "\n",
    "print((x < 3) & (x < 2))\n",
    "print(x[(x < 3) & (x < 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `where` function returns elements as it is if condition is met, otherwise changes value according to given formulae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1, 102, 103, 104, 105])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.where(x < 2, x + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique` function returns the unique elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.Tensor.item()` to get a Python number from a tensor containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a single item\n",
    "x[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can select a specific value or a range of values from a `tensor`, we can also assign new values to those positions. For example, below we have selected the first 2 items of `x` and assigned new values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 101,   3,   4,   5])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2] = torch.tensor([100, 101])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping & Resizing\n",
    "For some operations, the input tensors need to have a certain number of dimensions (also called rank) and a certain number of elements (shape). So, we might have to change the shape of a tensor, add a new dimension, or make a dimension smaller that isn't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `view` function can be used to reshape a vector. For example, before `x` was a 1 dimensional vector. Later we reshaped it into a 2 dimensional matrix with 3 rows and 4 columns. The number of elements in a tensor must remain constant after applying view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.view(3, 4)\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x_reshaped.shape)\n",
    "print(x_reshaped.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, `view` requires contiguous memory, in contrast to `reshape`. Therefore, `reshape` is safe to use in expanse of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x.reshape(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a tensor with dynamic size, you can use `-1` to represent any size. But you can set only one dimension as `-1`. It means that `PyTorch` will calculate the suitable number for that dimension by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate two tensors both row-wise and column-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates two tensors\n",
    "x = torch.rand((3, 3))\n",
    "y = torch.rand((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1190, 0.7488, 0.0051],\n",
      "        [0.1788, 0.9204, 0.8871],\n",
      "        [0.6160, 0.8255, 0.9660],\n",
      "        [0.0718, 0.4968, 0.0340],\n",
      "        [0.3249, 0.2817, 0.5476],\n",
      "        [0.7235, 0.0178, 0.3624]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y column-wise\n",
    "print(torch.cat((x, y), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1190, 0.7488, 0.0051, 0.0718, 0.4968, 0.0340],\n",
      "        [0.1788, 0.9204, 0.8871, 0.3249, 0.2817, 0.5476],\n",
      "        [0.6160, 0.8255, 0.9660, 0.7235, 0.0178, 0.3624]])\n"
     ]
    }
   ],
   "source": [
    "# concatenates x and y row-wise\n",
    "print(torch.cat((x, y), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide a `tensor` into multiple chunks using the `chunk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1190, 0.7488, 0.0051]])\n",
      "tensor([[0.1788, 0.9204, 0.8871]])\n",
      "tensor([[0.6160, 0.8255, 0.9660]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.chunk(x, 3)\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might need splits of different sizes. We can use the `split` function instead and define the size of each splits manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1190, 0.7488, 0.0051],\n",
      "        [0.1788, 0.9204, 0.8871]])\n",
      "tensor([[0.6160, 0.8255, 0.9660]])\n"
     ]
    }
   ],
   "source": [
    "x_chunks = torch.split(x, split_size_or_sections=[2, 1])\n",
    "for chunk in x_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1190, 0.7488, 0.0051, 0.1788, 0.9204, 0.8871, 0.6160, 0.8255, 0.9660])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unrolling all elements\n",
    "x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "torch.Size([64, 196608])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "# converting 2d color images into a vector of pixels\n",
    "print(img.reshape(64, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "# swapping dimensions, changing colour channel dimension to the last\n",
    "print(img.permute(0, 2, 3, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `squeeze` function removes the given dimension, whereas the `unsqueeze` function adds an extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 1])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# add and remove a single dimension to the existing one\n",
    "x = torch.arange(10)\n",
    "print(x)\n",
    "x = x.unsqueeze(0)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(2)\n",
    "print(x.shape)\n",
    "x = x.squeeze(0)\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classification\n",
    "In this short exercise, we will be building a naive **baseline** classifier to classify digits $3$ and $7$. We will utilize the `fast.ai` repository's MNIST SAMPLE dataset that only contains digits $3$ and $7$. A **baseline model** is a simple model that you are sure will work well. It should be easy to apply and test, so that you can test each of your better ideas and make sure they are always better than your baseline. If you don't start with something reasonable, it's hard to know if your super-fancy models are any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/musab/.fastai/data/mnist_sample')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads MNIST dataset from Fast.ai repository\n",
    "# This trimmed dataset contains only 3s and 7s\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast.ai API returns the path to the downloaded dataset. The path is not just a string, rather it is a python Path object Learn more from [**documentation**](https://docs.python.org/3/library/pathlib.html). Or watch this [**tutorial**](https://www.youtube.com/watch?v=YwhOUyTxXVE) for a quick introduction. The new path object comes with many unix commands baked into itself for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\labels.csv\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\valid\n"
     ]
    }
   ],
   "source": [
    "for p in path.ls():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `/` operator is one of the most convenient shorthands for creating new paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\3\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\7\n"
     ]
    }
   ],
   "source": [
    "for p in (path/'train').ls():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\3\\10.png\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\3\\10000.png\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\3\\10011.png\n",
      "\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\7\\10002.png\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\7\\1001.png\n",
      "C:\\Users\\musab\\.fastai\\data\\mnist_sample\\train\\7\\10014.png\n"
     ]
    }
   ],
   "source": [
    "threes = (path/'train/3').ls().sorted()\n",
    "sevens = (path/'train/7').ls().sorted()\n",
    "\n",
    "for p in threes[:3]:\n",
    "    print(p)\n",
    "print()\n",
    "for p in sevens[:3]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images of 3:  6131\n",
      "Number of images of 7:  6265\n"
     ]
    }
   ],
   "source": [
    "print('Number of images of 3: ', len(threes))\n",
    "print('Number of images of 7: ', len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are now in `tensor` data structure, we can use the `show_images` to display multiple images together, or we can use `show_image` to display a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAADeCAYAAAAJtZwyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASQUlEQVR4nO3dfZSWZZ0H8HtmAGGQEV9QgZUBnRkRX7DUDMvC2tYStKxV1k0N0tDFTiql7XGtbJdT5gtaoWJqmK5ma26aUe2xXNw2QAQ0ToTg8GIK8hIgqAODPDP7x26nU2d/18DDMDwXfD7/frleDpx7nvlyn/P8qtrb29sLAAAAyFT1nr4AAAAA7ArFFgAAgKwptgAAAGRNsQUAACBrii0AAABZU2wBAADImmILAABA1hRbAAAAsqbYAgAAkLVuO/oHP1R93u68B+w1nmp7dE9fIcmzDDumkp9lzzHsmEp+jovCsww7akeeZW9sAQAAyJpiCwAAQNYUWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWVNsAQAAyJpiCwAAQNYUWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWVNsAQAAyJpiCwAAQNYUWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWVNsAQAAyJpiCwAAQNYUWwAAALLWbU9fgPJt+/ApyfywLy0Ns4cG/yLMNrZtCbMLLvxs8szqZ55P5gDAn6vpe0CYLbnjyDBrPmNact8hT34mzI65fkWYldatS+4LUIm8sQUAACBrii0AAABZU2wBAADImmILAABA1hRbAAAAsqbYAgAAkDXjfipATV1dmC36xtAwmz/69uS+ddU9w2x2a+I+RbzulNvnJc+c/+5eYdbemjgUACpcdZ8+Ybb+E8cl12792Othdu6QBWH2037PhFmpvS15ZvPou8Ps0ZEHh9kD7z05PtMoIKBCeWMLAABA1hRbAAAAsqbYAgAAkDXFFgAAgKwptgAAAGRNsQUAACBrxv1UgJeviEcENJ8zJczWt7Un9x360BVh1jT1tTBbOrZ/mC285I7kmadeFJ958L2zkmuBnVfTdFSYLZ7QL8yOPH5lct87Gh4Js6vfc36YbX81vS9Uug2fHhFmIybMDbOf9E9/PpYvfgcxeWNjcuWm7bVh9tV+vwmzB3vHo/sK036oIDXHHh1miybG47l++sFvhdkxPeLnpiOPvRmP8Hxi/Ylh9tx/pMeFDbphZrlX2qd4YwsAAEDWFFsAAACyptgCAACQNcUWAACArCm2AAAAZE2xBQAAIGuKLQAAAFkzx7YSnLyprGXv+f4XkvmR18ZzY7cnV8ZzbDuy9ZCqstfC3qzm6IYwe3X0ocm1fzV6RZjdOOShMOtTFT/p81sHJM88qls8x3LVOfVhduid5tiSt5ZRm8Pstv7PduFN/tcfSm+F2fQvfiC5dtv+NWH21cnxHNuXxg8MsyHX/T55JnSl0sLFYdZ0SbzuquK0ss+sOSz+zH57aPzsNF8cP4/Lx9+ZPPOYYkKYmXH7J97YAgAAkDXFFgAAgKwptgAAAGRNsQUAACBrii0AAABZU2wBAADImnE/FaD+0lVhdk7Ps8LsqD/MS+7bXuZ9zh49u8yVkIfq3r3DbOOj6XFXa1b1DbPhja+E2dfrHwizAd3SY7Keb43vO+b+iWFWP/2NMCvVpn/8f+zh+8Ks+6h18cL0xAKoeC3ra8Ns1OKzw2zxknjMR1EUReP3WsNs0sP3htnnrv98vOmBySOL0dfOSP+BQJ+Xy1oG+4TSmrVhVp3Imp5JbBpXgaIoiqJHeZNB9zne2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWVNsAQAAyJpxPxWgtHFjl5+57KYRYXb/ITeH2ca29FiSQT98LcxKHV8LOk1Nv35h9tp5jWE2d/iU9MbDy7vPlavOCLNlYwcn15YWLg6zQcXMMEuN/Np8UfwzoCPrVvYNsw6mj0DFaxr/XJilnqmmYmXZZ46758owaz1rS5g1nzGt7DO/su74MDvsB78LM5/lUJ6Wc08Ns8feXJZcO/Dh5jDzTP6JN7YAAABkTbEFAAAga4otAAAAWVNsAQAAyJpiCwAAQNYUWwAAALJm3E/Gqrr3SOYrvnRSmM254NYwq6uuDbPh3/5s8syBzfHoEehsqZE+zd/uH2aLTu9gpE/CBcs/FGZvXnZImLU1rwiz9tZ4nM+uSP39nPK5+cm1K0stYTZ0yhth1tbxtWCfVHXycWG27bj4eWseeX+YldrTT9wjb8Y/A3522/vC7MDXZyX3BXbekGsXhdkT609Mri2tWdvJt9k7eWMLAABA1hRbAAAAsqbYAgAAkDXFFgAAgKwptgAAAGRNsQUAACBrii0AAABZM8e2wrWfNjzM+t3ycnLt9Po7EmnPMFm6fUuY1f9r+sztyRR2TtV++yXzg554O8wW1d8fZtuLUpgN/fEVyTObrpgXh23rk2u72tqPNoTZkwNSPx+K4pdbDgyztgUvln0nqHSpebOLJ8SfnSc1rkjue/OgqWE2qFs8Pz71DqLhx5cnzxw26dUwO3ClWbXQ2WqOPTrMPnrwz8Lsxq9/MrnvQYXndUd4YwsAAEDWFFsAAACyptgCAACQNcUWAACArCm2AAAAZE2xBQAAIGvG/XSRmr4HhFn14/HX/E+q/06YHd+je9n3mbyxMcxOrV0aZqtHDUrue8jdK8u+E/ylqpqaZH5+vzlh9uLbrWF20Y0Tw6xpal5fqV/VvUeYnXb53DBL/f0URVF87crLwmy/4rmOLwZ7UPUJQ5P54om9w+zR998VZif22JVfm1IjfWIvtMbP6rCvdjCCb/Wass4EyrP40nhU3rAeq8PsoGl5/e5RqbyxBQAAIGuKLQAAAFlTbAEAAMiaYgsAAEDWFFsAAACyptgCAACQNeN+usjKsceG2fzGKYmV8UifezYdkTzzpl+MDrOma14Is9bn4jNHjn82eebC78ajR9rf3pZcC3+praUlmd91/AlxWB3/v12/lr3na/WXTjopzKb3vyPMzlt6TnLf/aYb6UNlazn31DC77/bJybVHdesVZjVV8edYqb0tzB5/q2/yzC88/Xdh9quz4vt+cVm8rlj9avJMyMGS+04Os+rN6apy5OPxOKytB8XP8ta+5b/b23BCe5gtHTM1zIbNHB9mRxS/Lfs+/Ik3tgAAAGRNsQUAACBrii0AAABZU2wBAADImmILAABA1hRbAAAAsmbcTxfpvToeETB1U32Y/eCV+CvQ+1z6dvLMxlfi0TzxF5UXRXUivbX//OSZH+l5enymcT90sratW/f0FbpEzdENYXbrx78XZmtL8bikrZf17eDUdR3ksGf1WfJ6mF2wYFxy7byT/i3MHnuzLsz++c4Lw+zw22cmz2wq5oTZxX99ZZgNmbQ4zP77B4mRZ0VRDB6zIJlDV1l99Wlhtvwjd5a/8Zg4WrQt/gz8+msfDrNfzx6WPPKmUQ+HWernx+Ar1oZZKXkiO8obWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWVNsAQAAyJo5tl2kzyOzw2z6z48Ms16vLw+z7bt0o1hbURVm31jfmFzbvs2sWuhsa26J/w9yVO2bYdbw86vCrGnR3F25EuxxpYXxfNdD/2locu2J758QZod9O55He3iRnlVbru6/mBdmz044NsyGD1yZ3HdT2TeCznX4bfGzc/qKy8Ksz8z49+CiKIrSmng2bNrmMGko4t/Zi6IoFn1wQJhN++XIeN816X3Zdd7YAgAAkDXFFgAAgKwptgAAAGRNsQUAACBrii0AAABZU2wBAADImnE/FaD0+h74Qv53HR9GF/e9K8zOvu3a5LaHt+6eUQiwN2ue/O5kvvSkqWH28eYzw6zpEiN92De1LXgxmR+2oIsu0gkmHf9EmH3h8YuSa48qZnX2daDT1f7o2TArdeE9/mjDuBHJ/BN1t4bZr67u2dnXYSd4YwsAAEDWFFsAAACyptgCAACQNcUWAACArCm2AAAAZE2xBQAAIGvG/eyjTvnOC2E2sKY2zHqta9sNt4G9X8u5p4bZM397S3JtqT1+JpdMbwyzgcW6ji8GVLR7V54eZp8+8+nk2meu6dXZ14G9Xu8LXkvmo380McwaitmdfR12gje2AAAAZE2xBQAAIGuKLQAAAFlTbAEAAMiaYgsAAEDWFFsAAACyZtxPzqprkvHrn3xXmH3u4Hi8yINvDAmzA3/4QvJMw4DYpyWeySHXLgqz/okRW0VRFOctPTPMBt37YpiVkrsCXal9xPAwu/R7j4fZ+fu/EGbHfXNC8syBxcyOrgX7pLb3vyPMZhw3Lbn2rKvHhJnP3T3LG1sAAACyptgCAACQNcUWAACArCm2AAAAZE2xBQAAIGuKLQAAAFlTbAEAAMiaObYVrqpb/E+06sp4Tm1RFMXzE6eE2Zb2eN7mtGs+FmY9t85Jngn7stYz3xlm0wbdHWYvbNue3HfTDYPCrNv6eR1fDP5PzcEHJfPmzx8dZo3fWhZm21evKftOe4vq2vQ86leuiZ/z8/ffVNaZPTe0l7UO9nX9b1waZsNmXphce8TC33b2degk3tgCAACQNcUWAACArCm2AAAAZE2xBQAAIGuKLQAAAFlTbAEAAMhaRY/7qR5+TJitfXffMDv0wd8k921raSn3SrtFt8HxKI/Sd0th9vzR8Tifjrzr7olhdsSTM8veF/Z2NX0PCLOLJj9Z1p6fmnpVMh/4tGeSzvHq2KHJfNGn4s+VGWO6h9nXPjM2zLo9nddIqpqGIWG26R2HhtkRV72U3Pcngx8Ms1Jiak/Dk5eHWdM9s5JnAv+/B+r/K8xOv+myLrwJnckbWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWavocT/rTukbZnO+fEeYrb9+S3LfEY9+PswaH3ojzNrnLQyz0sh3Js9cfk6PMJt73uQwq6vuGWZPbemVPPOGG8aF2REPGREA5Xjl0mPDbGzdf4bZV9YND7P6B5Ylz9ze8bVghxw6b2syv3LViDD75oDE58Y994fRP04anzyz35wNYVZauDi5NpIa2VMURfHS+MPD7OZz47E8Z9duLus+HUmO9Ll8zm45E/Z2q68+LcwufrkuzGp/9OzuuA5dwBtbAAAAsqbYAgAAkDXFFgAAgKwptgAAAGRNsQUAACBrii0AAABZq+hxPwd/N/6K+5Eb/iHMbr11SnLfJWPuDLMVn2gJsxktDWH28f3TZ6bG9mxsaw+zb26Mz/z5Z05PnnnArNnJHNh59aOXl7Vuxr/EYwd6v2a0AF2jZsb8ZL7s4sYwu+Ce/cPs+0OeCrPZk+LxfEVRFL9ujf+PfdZb8X1Sjuv1s2T+4V7xZ31NVXyfUvxxXczY2j155lULxoTZMdcvjc9M7gpEzhv3dJjd99x7w6ypmLs7rkMX8MYWAACArCm2AAAAZE2xBQAAIGuKLQAAAFlTbAEAAMiaYgsAAEDWFFsAAACyVtFzbIu2eHpb7b/Hcx+/vHxsctuXJvYIsyUfuC/MxtatSuwaz6ktiqI4t/msMGv5yoAwS80crCp+kzwT2HnbP3BSMn+sITWTM/7ZAjkoLXopzN74m9owO+b6K8Ls6QtvTp75vp7xfNz37Lc4uXZ3GPf7eEZ88zeGhVmfmekZ1wPW/C7MzKqF8mwYNyLMrj/krjCbed2gMPM85ssbWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWavscT9lan9+YTJvuCjOzire2cm3+aPVYVKTyIDOV1NXF2Yn3BKP2CqKouhVFY/0OWHKZ8Os/qnfhpnRAuSgraUlzIZcNyvMLrnuvbvjOrvRG2FSW8SjBj3H0PXeGhU/rxe//L4wK61Zuzuuwx7mjS0AAABZU2wBAADImmILAABA1hRbAAAAsqbYAgAAkDXFFgAAgKztleN+AJJqasJoZN2i5NLZrXE2+JGVYbZ98+YOrwUA7Li/b5obZk/cdkaYHVTEI8rIlze2AAAAZE2xBQAAIGuKLQAAAFlTbAEAAMiaYgsAAEDWFFsAAACyZtwPsM8pbdwYZt9qGLoLO7+8C2sBgJ3xqxN6hpmRPvseb2wBAADImmILAABA1hRbAAAAsqbYAgAAkDXFFgAAgKwptgAAAGRNsQUAACBrii0AAABZU2wBAADImmILAABA1hRbAAAAsqbYAgAAkDXFFgAAgKwptgAAAGStqr29vX1PXwIAAADK5Y0tAAAAWVNsAQAAyJpiCwAAQNYUWwAAALKm2AIAAJA1xRYAAICsKbYAAABkTbEFAAAga4otAAAAWfsfcJ9AyaB7tDcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images((three_tensors[15], seven_tensors[10], three_tensors[19], seven_tensors[100]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are now `Python` `list` of tensors. However, we need a single tensor that will contain all the images. The `stack` function of `PyTorch` does exactly the same thing. It stacks individual tensors in a collection into a single tensor.\n",
    "\n",
    "At the same time, we are converting the data type to floats since deep learning algorithms will yield fractions. Also, we have normalized our pixels values by dividing them by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "torch.Size([6265, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(seven_tensors).float() / 255.0\n",
    "stacked_threes = torch.stack(three_tensors).float() / 255.0\n",
    "\n",
    "print(stacked_threes.shape)\n",
    "print(stacked_sevens.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to find the mean representation of $3$ and $7$. Then for any new digit,\n",
    "we will compute the distance from the digit to both $3$ and $7$. We will predict the\n",
    "digit with the smallest distance between the two. Next, we will calculate the average\n",
    "representation of the digits $3$ and $7$.\n",
    "\n",
    "When we are calculating the mean, we are calculating it for each pixel across all available images.\n",
    "That's why in the `mean` function, the given dimension is $0$ which is the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAX8klEQVR4nO2c25IjyZGeP/eIyEwAdegDhzMaimtjutCF7vYh9hH2KfUIehHpShSXtLXlzHRXdxWAzIyD6yIiE6hmkxwzFprVa+VmMKAAFJBwDz//7mJmxgv9Q0n/0RfwQi9CeBb0IoRnQC9CeAb0IoRnQC9CeAb0IoRnQC9CeAb0IoRnQP6XvvFf9F8veR3/Kel/lf/5i973ognPgF6E8AzoRQjPgF6E8AzoRQjPgF6E8AzoF4eoz45E/vrrX1Gv6nkJ4VPGirY7efQ3APo3hFDOhGDl7KH92XP/aIE9HyGcC0D0MePbY1neo/r5/4MTQx1QSnuqvacYou25cm6J/7EC+bJCkMcnWlRWJotIZa4I4hw4V5nvfX1N6mO0PV4/60wICwPNIJd2n7FSqmaUDDlDMSzXx2ZWX7PStKQ8/qwvQJcXwhnjF6aL08pw5xDvwWm97ztQxbqADQFTpQwe84p5IXeKOcFUMA8mAucKVAzJ9V6jIdnQuaBTqq+NCZkjlIKOM6SE5QLThOWM5Fz/XgRiX0YglxXCZwSASmX+ctq7gKhCF7C+A6eUoaNsPOaUtHWUoJiH1CvmoDiheEDAzoSgGTDQbLjJ0Aw6G/6gSDGcV/QokCtTZRLEFaxkpP5rZXhRIF+UNed0GSGcM9+5+jD4xnyFvq8nP3hs01OCw/pAugoUJ6QrR9wqxQtxB7kXSoA8QPFg3igBTO2xJmRBSr13R0Uy+BHCg0MShENH95CRDOF+QMeIxIzeB4gJYkSOY9MKxVI6magLasOFhNBsvnOV+SLI0CMhgPfYdqB0ARs88aandErcKdOtUgLMN0K8NkoH8SYj24yGwvXVkT4kNiHyuj/gtaBieCkUE8bsmYvnmALv9lvm5Ljf93AX0CiED0r3QXGz0d85uocOnQvdEKrJOs5VK1OuJsoMEcMyXFIQTy+EpgWisjpRcQ7UVcfqXdWA3lE6R+6V0gtpENJGKB2kLaSdUTpDrhLDdqYPiV9fPbALE1dh4tv+niAZFSNIpiAccsdUPPvU07vEMQV+doX7LORZkShoFMwLbgLNilMonas+I/kaCJhhziEi2BewTE8rBJGTCVJBvEeGHtQh2wEbevCOdDuQN548KMc3ntxDvBamV1A6I95m3G0k+Mybmz1vNwe2fub7zQeu3MStP/CNr0IIkuikcmk2RzTPvvT8++YVh9Lxx90rfje84Rg9d9sdx02PzkIJVfB+FKDDHz3+weFLQSZffURKSIusLJX6+y6gDU8nhDMNWKIf6bqTCRp6yrbHeke86Yg7JW2E8Y2QNxCvjPlthlAYbie+vb2nd4l/unrPN909V27i++491zqy04k37oGOQi+ZQcqjSxlNedcP7EvPvw1v+d/Df+FjGvg//bf8e3dDmh2j9JSgpFHQ5Mhdjbp0SohzaCnINNcwVtMpUbSnV4sn1oR2oS3eXyKh9bHXGl46TqGmqxGOOaqTVRA5nbZiQjRHNMdYAtri+CCJIJmBSCTjMLTGN0SUjOKk4KQQJNNroneJEDIlKzlACYakGmkVv1yXIq6crtlqDnPJIPUCPqHF/yLND9RQ1DpP6RylU3Kv5A5yR433FwEAFEjR8XHs8S5QEN7PG7wWfuff0mmqDNW0MnfrJoJktjozyAyAa4K8z5vVb2x8ZNvPiBgPm0BKDhNI2/rlLiqhryyRrmpwFUZEVE4ljyemJzRHujpjETmFo85h3mFeKaHdvJxOnlYh1BgdMCFnZYqeVAowcIyhMtJlvBREjM5lFGPwkY2LeCm8CgeuFoG4CYcxmsdJwWth8JHBJ8yEfZ8psyJZyD1IhtwJpVOk1MMjqpXxj0oqT+8XvkzZol20WM1iJRuaatKlczVHToVybMlYgWMWRI3RF9SVeiC1tIqFoe3Wh8TgE04Lr/sN12Gi08SrcGTQSCyOqXiiOdJ5vUhYk716E6xpz5oAqoI+9jeXoMsIYQlP4SSAVJBYUMCPpWpOBoSaiPWCG7X9rZTgVyaV9lGrS1wyZYGPnVG6As5w15FhiHQ+8c1uz8ZHOpfotP7nIXUUW4qBRlEDaSf9TChQhfLZOu2zjo4aiXzm0tfCGpBrfUeTYQIapZkhqxxQKLE67j//nMoz05MQSifkvpq2ZMIhKXPwdD4zF0fvEldhAiA/qpyyMv7PuP23ehVPTE8uBDNDSuWWmSEpI2YwJ9TX0NW8IkVRX8sM1mpBOTQGNz/xF79Dl+gK8gYoUiOdKFhSijNy0VMJG9DF1AC5CFYEScutHgpNte4kqbT8oDzuS1yInt4ctTKxAKRWci7VySmAKr4ULDhMhbBvYatWZ43Ieso/PaFL1bQEIYd6+ucrQa6kasSg5GAUV8hFKCaPBAFVG3JRLCkaBU3gZtAIGlvVNWaIqZXA81dURbWCFUXaCV40ot63U5XryZJ4atIUqwmeqSBZml1eCoBnHy9nJsoUUyi0gp3BGsi3E78473MnXtoHGkCp/0u71RI49eRna9XU5VYuFp7CJTTBSm2cmGE0Bou0yKhUh51zLZS5pcRB1QDV+li1aoMK5pYET8gtwSudEDdK8bXcMd9C7o38KtHdTPR95NurB266EcXwmimmFBOOc2CePTIp7ij4gxAORjgU/CHjjhGZEzJFLKVT42f5bRegi0RH1rpaAthcT7zlpgmAeHfWRdO1U2bL4yYgU8V6VwURarEPhRyEtKlOeb6G+bZQhsLwauTt9Z5dmPnN9gM3/lhDU3NM2dfsOzlS9OhR8QfBH3kkBJkiMkWIJyFw3uC5AD29Yy62miRKWU+4WLOv0pyinSVBckrybAlgpEZKiwBKpzXb7lrFddt6DFujbAr0me0wcdOPbP3Mzk9sXESLkXK9oFyUnJWSBZ9pURotd6FqasrV/OTFMZeLCgCeWghW4/9qksBEqxXOGSsOcmnljIQtXbalh+x9a+ALFjy2CRSvxJtA3ihpUI5vlNLDfA3T24L1he71yG9f3bMNM//t+me+7+8IkrlyI0Eyf4o3fExDLXHPgfkQYHT4ByHcQ9gb4SETHhJuPyPjDDFCnKsmFGutzq/BJywnuzloANFyKjqatZN+MjlLTmHSSh2hXY6TmrB1jrRzzDslbWB6DXljxNuC/+ZI3yd+c/uB/37zJ67cxA/DT3zn7wAoKNmUQ+kASKbE7GBy6Ki4EfzR8KPhD7l22caIza33HJspgq9ME34JlVIrq3BCTrja8LEugHfkbUe8DpSgTDdKvBLSFuZXRtkW5Gbm7e2eq27mt7s7fhh+ZqsT3/k73roHMsqh9GQRHLY65VL0USSkDRRQAQKfRESNLhkVLfTE5mi54HNsT66FvaIYVm3/mWqLr90s2/SUmw2lc4y/6hhfO3IPh++kOt5t4ea7e15vj3y/+8A/3/6eW3fkh/AjP4Q7AsZOhYCwt8J/5Jm9dQRJJFPm7InRobOiSVpuYLhoSMztlrAFFpMv3E47o8tqQvMRtoCuROspWxx3K3vjFLxbS91pUOJWyENt9pSbRNhFfnP7gW839/yw+Zn/MfyRV3rge3fge9+jKM0DEWzmQ8lEy2umXJB6RpomUGiggKoNtWxStWAJSb+EFsClhHAe+XxKS99ZZGW+BU/ZBNLOkzsl7oR4A7mHfJMYbieutyO/3b3nu/4j/7V7xzfunmuJ7FRWASQy2YzRMqMFZhzFpOYKkvG+kDoj59rLzp2gSWqPOdbeN761NnO+aA/hnC6nCWeO+hGGFB4Bv6zvsC6Qtx3ztSP3wvRamF4beVvY/urAb1/f8evNPf989Xu+D+/5tbvnBz/TizJIIIgjWyFaZrTMwWof4VB6MorXTOcyfUgch0wB0saRNiAm5MEjqeBSQbyrSWbOWEw1uCh6kbbmyo6LffI5fRJdyHl+sOYD0gp5rd0YDPNG5zODS2xcZNDIILGiLNpnZTOiZRK5aYAxmhDNM5ujtMRDMVRryduc1b6FF0orBJqr6L4VfrmiRS7Poi8THZ39kBOo95QZo7U0UVqnDTnVg+bk2KeOn6cd/3f6hvu84dbt+THfEyQB4DAywmjXK/Pv8o5ojndpV3+oZnZdZL+bid4Trx2SK/xlfvBVAELFHnlX/dgSplo5FQK/hn7CL6IzUK+prmXp5baQFCElx8NcY/0/HF/zIWzo9TW/d+Pa2IfqeKM5cvuA2GrhU6k/MUjhqpsYd55jCDzcBsQc5oT5o66Cd/seRkVSrgmb6il0/ZpqR4/ob8wUiFl1G+fRSgaJgiik2bGfOnJRfvRX7HNHp4k7t1mb+Qtlk5P5aTCYJUeI7Xmntd+MM4o3Smjlj9D63l5rbctp7f6JnUQtl/ENl8eifm7AY6Fc6/Yigpsy3UN1zOWuZtU5CDEOPHzouPfGT8MN4k/95tP3VTY5V0vXzhWGkPAuM/jEdTetWnPVzTgxHnaRZLV5NN0qxYGYI+w7zCkuZWSasVyQsyKe2dfQ6P8roek6gwBncXn9kTpn3FSQIvi+OmfnBSlCPjrMWQUOKyC29p3Xno1ACtXp4ozjLqEusxkiAJ1mnBZ6V/3IsJk5mpCLkDdaTd8IuXdQQHuPhIBIqogRTRdDaz89DBLWUy9/I7qw1mMwLRUdPRcoQjhqi5QMEDQCWgVTu26yNnwWR25SIZSm9T4DOSgiMPbV8QYyvUs1UpIaLWVfGtKbFQSm/ixoKKchFrsQBOzJYZCfziI8GnE61wSoKh4b3hMIgHnFjZnwsTrN0iklyNpZOz/5Jp/0m/uagOVemG9rQjbfOu7U6ELiepjY+Fg1IiRKwzjlwZAiuI2QNs2njAHZ18StVnhjS+KUp0ZoXwYG+cgZL899RgBS1hqNxASzg6S4hk+qaAq3tjXXbtuiDbDG+6YV1Z07SLH1oQuUXonRIWLks35zcJmoDtWaj1RtaLlKkFMIrdVBXxKB8TRCWNDY5+NQsGbG9S1nyc959XShUmCOiCpSqnlCBJ30UbsTTg1/AAtKcXWcSkyRXO1TmquWSBQstxNflFT0EfZItFRT38Lj4kC15g84rdnyIhCzWsp4Yrfw9wvhfB7hbBwKbcI4Z/oKGJbTMOASh+eC5Hn9TFn+5+w71tN/PjzoXUVueEVjjw4OzdrMipAnmGdHVojJrdgjAZwaqkZytsJuasZuTbCKWEWKVBTIZbThac3RWQQkC9PdmYk6Zz48Nk/nNnYBgn1KjwTekj1WpUBiQbqG7GvWjpZ5/2ITfu74zw+KysWGRZ7IHOlpGFCkIppF1l4BsJYn6vs/w+AWrq4NlfNx2PJJpqpnn+WWVNvgHG7vaxJmAcTX3KFiWQ2zT6Dunwh8wad+KSTek2nCcsJlKQerVgEEX83H0jeobz7948LgXGqpQBvT8ymPWASy1PlleV61hrjOasDSioHFLVkwFG9IAxU7XfBH7auNs5rQX/xhT8Wiv0h/nxA+vcAWScjikJ1i3p1stzuz5QuVxUnKOsIq+ez1LKekboGlL05dpMLug6+IDF8jm+JZY3/z4FyptzZouJBZbfQsxcLlJTl7vGrkswZ/LVHROo+g0IWaZfaBMnStcX8SwlKxrD/cWpertRmNGhmlUl9rEBQWjCu17EwTbtl25KEOoExvAvOVEHdLP6J25W53I5susg2RThNz8eRSG/8lu3WgcMGkLvB9UsNKLQAwuwzy4ul8wnr622TOcjqHugqhJl16gjMuh73ZBs2GRm3CUDSWmlFHRVIVwoqE04pFMifkTagD550y74R4VWef086wbSZsZ26GicFHOpfxWkhma6JWsuCWomE+LySe4JuPMEgXoKf1Ce3eWkPEXDv9DcKSu2X+QB6VrIEGxGpCSG0tQjE0lgrKgtUkmFcs1B5E3Hni7mz++aoCwmyXcNvEZohsw0ynGW3zzsWElJWUFItaJ//TcltWMpQWNhfMPjkET0x/txDWKqlKnVFbNcFhvSMPdS1CBXBV5qd+2U1xphWlagPWENKpmqgFrg6szrMEadOWrCe/BJjeFMouI5vMN2/vue4nrsPE236P18w+9RxSIGbHOAfioUOODr8X/AH8wfCHghsLekx1ejPltgPjcnDIy1RRG6a0LgLRtW25wNmXNQlIdaC0cowUQRYhxJoraATNjwOA3CZ7zEG8aoPnAcpVxl9F+iHyZnPgVX9k4yLXYQRgLp5iPQUhJYVlPuFTTVjK17mclpB8FU2d5SKXErXZGVy9lgNKqEzPQy22mUIe7M8GQiQ3xtvJTsMpnLdQUdimkHcF22Y0ZG6uR15vjww+8pvtB3Z+OkFeTLiPPe/GLfumBbp3FZW9B7+voGB3SLgpI+MnoGC7HDz+7xbCAgA2s1P8nk8ou6XSWbysGhCvKpyxBCNfFfC106W+rFHT6fNb5iSGtIUizmf6PuG1cLsZedVXxn83fOS1P6zTm0EyH9KWP8VrpuL5MG/4+WHLPAX0ztPdKf4A/V0hHIzuQ8LfTxUafxixccRywXI+rd75KnrM55OaZjXkXLRY2k1pWS3gC9IV1Be8z6jao2Hy+pGydswE6Hxm180El3ndH/hVv6fTxK/CA7fuiErBccoJUnGk4jjGQIyOHBU3N1O33GZDUyscpraQao2KvgZovC2TOBlTV+GEgEweN2UwxUVFY8tqz8yLdAXfJYYh8mozElwb+vbzusElaJ1Z3riISqHXxJWbcFLY6sxWq9lx1An+sQTepSsm8/zh+JrfPbzhEAM/vruh/NzhRmX4SejvDH80hvcJd0j4hxk5jHVUam4rFZopqhf8TM3ROiaVc51DkASpzqyt3TIDN1vdsKK1gQKAGs4X+j5xM0z80/V7Ni7yTXfPr7uPDBJ55Q5stTJ8JzNBEo46ob9AXaCiK+7Kln3pmc3zPm25i1v+sH/FH9/dEmcPP/YMPyluhs2PxvAh40ajezeix4gcJ2x/rM54nisy20oTxHPNmD+FOy4DFaW2KSVnJLcB8DZAbtqcbaq1/pIrrCUVZS4OlcIC7VrunZS6u+Lsvr5eYS4zdffFz/mK+zzwLl3xH9M1d/OWd8ct86HDJiUcBT+Cm8CPhk5WnfCc6tKpnFn35Jld1Bmf05OZo2VECoBpQnId/nCqdXTW1UG93FeskZvrxM0kPeMmME+1lND7xCF1HDeBjYtMIXBbeoIkBh3oJK/IuozyLl3xLu04lI7/d3jD+3HLx7nnx3c35NGh957hp2oK+/fG8D7jotG/j7iPc9v8tYdpxmLEjuOaJds6n3BZQTyBOaraUMdmXT1FKbWtWdKqqQ4XHEEFnevIU80HhNILOTqSwQc/0HWZ4E7Qx9B2GQXJjBZxFKK5hjMV/ji95k/jNYfU8W8fb7nfD6QxoD8FulEI98Lws+Fm6D9k+ruIxIL7OKGL/T8c6v0yHLIuIvyaUNlmLbRsuJxc2tKmivkXq5sYNTgkGWGoPQBJLfs9gkZHLBuiL4xj4O6wwbvCq80bdn5GpaxLRZIpYw4UE96PG+7Hnhg948ceOdRJnP5d3e4V9kb/sZYiwkPGHVItFI4TzBFSNUU1I/7MJsgvQE87x5w5lXxF1owTp2hM6GHEvMM9DHXVZqfMPwZKGwacr7RugNwE5mHLpPCxNyy01GEJcc8Qe26sAx8hwXZvuBFcNLr7GhT4MeMeqtmR44wcpxr1zDOlredcR2W/4BrOc3r6mTWoP0gUITaYuaxwc3EONUOnWmWVWLCgpMHhj67uqOhP+ypy19Zvwlp5XeCSGLjJ8FOdvgzHghtr0c/v26bHMa2MZ5qxaa4HZo5rPehLhKF/jS4wLrUMDjYoSxPEurXX1XAW7xHvkDlhrvmMjW/D4rU0bdLalH7h/vI9rf9QTqsQpBg6ZXROdTfFGOvQekyV+UsdaJ5rRfR8JOoLmp7P0WUy5oYJMStYojZ+UhubhdOGYKmbIkUFEUUXEO75UPmnnbjz74HT9oC2knkdeUqpmpqF4XA69fCY8f+pF5afDRJa5tQzXBggWjGecMKpLm3OsxHbz26IX3oLa/uxPAIFrDuw2/c9x23xC33RzV/rVOcCHRGDXHsS1fE+htF/yqJ17vlT5n1mTb995rnH1/J86MsOiXzKgMVs/UKT/PzY9zT0ZWbWXuiv0osQngG9COEZ0IsQngGJXQrH8UK/mF404RnQixCeAb0I4RnQixCeAb0I4RnQixCeAb0I4RnQixCeAb0I4RnQ/weqny0E4/MsmAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create some functions to calculate distance between two images, and\n",
    "also to predict digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_distance(img1: tensor, img2: tensor) -> tensor:\n",
    "    \"\"\"Function to calculate average l1 distance or the mean absolute error between\n",
    "    two images.\"\"\"\n",
    "    return (img1 - img2).abs().mean((-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img: tensor) -> int:\n",
    "    \"\"\"Returns 3 if the distance between given image and average image of the\n",
    "    digit 3 is smaller than that of digit 7. Returns 7 otherwise.\"\"\"\n",
    "    mask = l1_distance(img, mean3) < l1_distance(img, mean7)\n",
    "    mask = torch.where(mask, torch.tensor(3), torch.tensor(7))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "a_7 = stacked_sevens[12]\n",
    "\n",
    "show_images((a_3, a_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(predict(a_3))\n",
    "print(predict(a_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model worked well on the training data. Let's load the validation data and \n",
    "measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float() / 255\n",
    "\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]) \n",
    "valid_7_tens = valid_7_tens.float() / 255\n",
    "\n",
    "valid_3_tens.shape, valid_7_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will combine the $3$ and $7$ tensors to a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_x = torch.cat([valid_3_tens, valid_7_tens])\n",
    "valid_data_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a tensor to store the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_y = torch.tensor([3] * valid_3_tens.shape[0] + [7] * valid_7_tens.shape[0])\n",
    "valid_data_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will predict and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(valid_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.14229583740234"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = torch.sum(predictions == valid_data_y) / len(predictions) * 100\n",
    "acc.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
