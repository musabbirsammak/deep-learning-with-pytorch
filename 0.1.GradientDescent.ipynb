{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent from Scratch\n",
    "In this tutorial, we will be building gradient descent algorithm from the scratch\n",
    "using `tensors` that we learned in the previous topic. It will test your knowledge\n",
    "on `tensors` and its operations. First, we will import all\n",
    "the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "A tensor can be created with `requires_grad=True` so that `torch.autograd` records operations on them for automatic differentiation. For example, we know that if some function is $f(x) = x^2$, its derivative with respect to its variable $x$ will be $\\frac{\\mathrm{df(x)}}{\\mathrm{dx}}=2x$. Therefore, for some random value of the variable $x$, say, $2$, the derivative of the function will be $\\frac{\\mathrm{df(2)}}{\\mathrm{dx}}=2(2)=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dervative at x = 2:  tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same when inputs have multiple data points. Now our function will be $f(x_{1}, x_{2}, x_{3}, x_{4}) = x^{2}_{1} + x^{2}_{2} + x^{2}_{3} + x^{2}_{4}$. Since we have $4$ variables now, such as $x_{1}, x_{2}, x_{3},$ and $x_{4}$, we can not simply compute the derivative. Now we have to compute the partial derivatives of the function $f(x)$ with respect to all the variables individually. Here the partial derivatives will be $\\frac{df}{dx_{1}} = 2{x_{1}}$, $\\frac{df}{dx_{2}} = 2{x_{2}}$, $\\frac{df}{dx_{3}} = 2{x_{3}}$, $\\frac{df}{dx_{4}} = 2{x_{4}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2., -2.,  3., -3.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2, -2, 3, -3], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x ** 2).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dervative at x = 2:  tensor([ 4., -4.,  6., -6.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "We will use the MNIST Sample dataset from the `Fastai` repository. This dataset contains\n",
    "a stripped down version of the actual MNIST hand written digits. It contains the digits\n",
    "$3$ and $7$ only. Our task will be build a `Logistic Regression` classifier to classify\n",
    "the digits. First, we will create dataloaders for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('C:/Users/musab/.fastai/data/mnist_sample')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads MNIST dataset from Fast.ai repository\n",
    "# This trimmed dataset contains only 3s and 7s\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/labels.csv'), Path('C:/Users/musab/.fastai/data/mnist_sample/train'), Path('C:/Users/musab/.fastai/data/mnist_sample/valid')]\n"
     ]
    }
   ],
   "source": [
    "print(path.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/train/3'), Path('C:/Users/musab/.fastai/data/mnist_sample/train/7')]\n",
      "\n",
      "[Path('C:/Users/musab/.fastai/data/mnist_sample/valid/3'), Path('C:/Users/musab/.fastai/data/mnist_sample/valid/7')]\n"
     ]
    }
   ],
   "source": [
    "print((path / 'train').ls())\n",
    "print()\n",
    "print((path / 'valid').ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/'train/3').ls().sorted()\n",
    "sevens = (path/'train/7').ls().sorted()\n",
    "\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "\n",
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes = torch.stack(three_tensors).float() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28 * 28)\n",
    "train_y = tensor([1] * len(threes) + [0] * len(sevens)).unsqueeze(1)\n",
    "\n",
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset = list(zip(train_x, train_y))\n",
    "\n",
    "x, y = dset[0]\n",
    "show_titled_image((x.view(28, 28), str(y.item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_stacked_threes = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_stacked_threes = valid_stacked_threes.float() / 255\n",
    "\n",
    "valid_stacked_sevens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]) \n",
    "valid_stacked_sevens = valid_stacked_sevens.float() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2038, 784]), torch.Size([2038, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = torch.cat([valid_stacked_threes, valid_stacked_sevens]).view(-1, 28*28)\n",
    "valid_y = tensor([1] * len(valid_stacked_threes) + [0]*len(valid_stacked_sevens)).unsqueeze(1)\n",
    "\n",
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_dset = list(zip(valid_x, valid_y))\n",
    "\n",
    "x, y = valid_dset[-1]\n",
    "show_titled_image((x.view(28, 28), str(y.item())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We will use a 2 layer neural network where the first hidden layer has 16 neurons and\n",
    "the second hidden layer has 8 neurons. Our images are of size $28\\times28$. \n",
    "Therefore we will need a total number of $784\\times16$ weights for our first hidden\n",
    "layer, in addition to $16$ bias parameters. In the second hidden layer, we will need\n",
    "$16\\times8$ weights and $8$ bias parameters. Finally, in the output layer, we will\n",
    "need $8\\times1$ weights and $1$ parameters.\n",
    "\n",
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=0.1):\n",
    "    return (torch.randn(size) * std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = init_params((28*28, 16))\n",
    "b1 = init_params(16)\n",
    "w2 = init_params((16, 8))\n",
    "b2 = init_params(8)\n",
    "w3 = init_params((8, 1))\n",
    "b3 = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 16])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 8])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(w1.data.shape)\n",
    "print(b1.data.shape)\n",
    "print(w2.data.shape)\n",
    "print(b2.data.shape)\n",
    "print(w3.data.shape)\n",
    "print(b3.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "We will define some useful activation functions that will be used in the model. We\n",
    "will use `relu` in the hidden layers, and `sigmoid` for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.max(tensor(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    x = x @ w1 + b1\n",
    "    x = relu(x)\n",
    "    x = x @ w2 + b2\n",
    "    x = relu(x)\n",
    "    x = x @ w3 + b3\n",
    "    x = sigmoid(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "We will need a loss function for gradient descent. In this custom loss function,\n",
    "we are simply calculating the distance from the prediction and the actual target.\n",
    "Since the prediction will be ranged between $0$ and $1$ due the `sigmoid` function\n",
    "in the output layer, we can safely avoid `MSE` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets == 1, 1 - predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.49750778018211833, Validation Accuracy 0.4931640625\n",
      "Epoch 1: Loss 0.49600484602305356, Validation Accuracy 0.4931640625\n",
      "Epoch 2: Loss 0.4940931876094974, Validation Accuracy 0.4931640625\n",
      "Epoch 3: Loss 0.49158884797777447, Validation Accuracy 0.4931640625\n",
      "Epoch 4: Loss 0.488229726650277, Validation Accuracy 0.4931640625\n",
      "Epoch 5: Loss 0.48363932845543844, Validation Accuracy 0.4931640625\n",
      "Epoch 6: Loss 0.47726928579564, Validation Accuracy 0.4931640625\n",
      "Epoch 7: Loss 0.46838827218328205, Validation Accuracy 0.4931640625\n",
      "Epoch 8: Loss 0.4561115849991234, Validation Accuracy 0.4931640625\n",
      "Epoch 9: Loss 0.4397937451698342, Validation Accuracy 0.4931640625\n",
      "Epoch 10: Loss 0.4197216477929329, Validation Accuracy 0.4931640625\n",
      "Epoch 11: Loss 0.3976158603113525, Validation Accuracy 0.4931640625\n",
      "Epoch 12: Loss 0.37578730862967824, Validation Accuracy 0.4931640625\n",
      "Epoch 13: Loss 0.3557029231166353, Validation Accuracy 0.5748936102642277\n",
      "Epoch 14: Loss 0.33780731275981785, Validation Accuracy 0.7922780106707317\n",
      "Epoch 15: Loss 0.32159867350544247, Validation Accuracy 0.855762512703252\n",
      "Epoch 16: Loss 0.3064578203674482, Validation Accuracy 0.884301162347561\n",
      "Epoch 17: Loss 0.2916186489164829, Validation Accuracy 0.9065120045731707\n",
      "Epoch 18: Loss 0.27640018688172713, Validation Accuracy 0.922704681148374\n",
      "Epoch 19: Loss 0.2598146736925962, Validation Accuracy 0.9350506542174797\n",
      "Epoch 20: Loss 0.24073518926695903, Validation Accuracy 0.9444669397865854\n",
      "Epoch 21: Loss 0.21813666972578788, Validation Accuracy 0.9528471163617886\n",
      "Epoch 22: Loss 0.19165778342558412, Validation Accuracy 0.9568724593495935\n",
      "Epoch 23: Loss 0.16292127024154274, Validation Accuracy 0.9588652820121951\n",
      "Epoch 24: Loss 0.1360497207437851, Validation Accuracy 0.9593734120934959\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "\n",
    "    for xb, yb, in dl:\n",
    "        preds = forward(xb)\n",
    "        loss = mnist_loss(preds, yb)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        w1.data -= w1.grad * lr\n",
    "        w2.data -= w2.grad * lr\n",
    "        w3.data -= w3.grad * lr\n",
    "\n",
    "        b1.data -= b1.grad * lr\n",
    "        b2.data -= b2.grad * lr\n",
    "        b3.data -= b3.grad * lr\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        w3.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "        b3.grad.zero_()\n",
    "    \n",
    "    acc = []\n",
    "    for vx, vy in valid_dl:\n",
    "        v_preds = forward(vx)\n",
    "        v_preds = torch.where(v_preds >= 0.5, 1, 0)\n",
    "        acc.append(torch.sum(v_preds == vy).item() / vx.shape[0])\n",
    "\n",
    "    print(\"Epoch {}: Loss {}, Validation Accuracy {}\".format(epoch, sum(losses) / len(losses), \n",
    "            sum(acc) / len(acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
